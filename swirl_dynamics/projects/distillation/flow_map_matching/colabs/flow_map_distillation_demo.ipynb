{
  "cells": [
    {
      "metadata": {
        "id": "D1w2EysY6R7D"
      },
      "cell_type": "markdown",
      "source": [
        "# Flow-map Matching Distillation Demo\n",
        "\n",
        "In this script, we provide a simple example of how to use flow-map matching for distillation.\n",
        "\n",
        "In this case, we will train a conditional rectified flow model, instantiated using the stochastic interpolant formalism, to generate numbers from the MNIST dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### Training the Teacher Model\n",
        "\n",
        "We use a rectified flow to bridge two distributions, namely $\\rho_0$ and $\\rho_1$, where the former is a normal distribution and the latter is the distribution of handwritten digits from the MNIST dataset.\n",
        "\n",
        "Thus, we consider a straight path between these two distributions given by:\n",
        "\n",
        "$$x_t = (1-t) x_0 + t x_1,$$\n",
        "\n",
        "where $x_0 \\sim \\rho_0$ and $x_1 \\sim \\rho_1$.\n",
        "\n",
        "Following this path, we train a rectified flow model by minimizing:\n",
        "\n",
        "$$ \\min_{\\phi} \\mathbb{E}_{\\rho_0, \\rho_1}\\mathbb{E}_{t \\sim U[0,1]} \\| v_{\\phi}(x_t, t) - \\dot{x_t} \\|^2,$$\n",
        "where $x_t = (1-t) x_0 + t x_1$ and $\\dot{x_t} = x_1 - x_0$.\n",
        "\n",
        "---\n",
        "\n",
        "### Distillation Through Flow-Map Matching\n",
        "\n",
        "Once $v_{\\phi}$ is trained, we use a flow-map matching approach to distill it.\n",
        "\n",
        "In a nutshell, we consider the flow-map $X^{t, s}$ as the map such that:\n",
        "\n",
        "$$ X^{t, s}(x_s) = \\int_{s}^{t} v_{\\phi}(x_{\\tau}, \\tau) d \\tau  + x_s = x_t.$$\n",
        "\n",
        "I.e., along a trajectory, it maps $x_s$ to $x_t$.\n",
        "\n",
        "Now, following [1], we take the derivative with respect to $t$ and use some elementary calculus to find that:\n",
        "$$\\partial_t X^{t, s}(x_s) = v_{\\phi}(x_t, t)$$\n",
        "\n",
        "Then, by replacing $x_t$ with $X^{t, s}(x_s)$, we find that a flow-map needs to satisfy:\n",
        "\n",
        "$$\\partial_t X^{t, s}(x_s) = v_{\\phi}( X^{t, s}(x_s), t).$$\n",
        "\n",
        "This is called the Lagrangian formulation of the flow-map. We then proceed to train the flow-map by softly imposing the property above, i.e.:\n",
        "\n",
        "$$\\min_{\\theta} \\mathbb{E}_{\\rho_0, \\rho_1} \\int_{[0,1]^2} \\| \\partial_t X_{\\theta}^{t, s}(x_s) -  v_{\\phi}( X_{\\theta}^{t, s}(x_s), t) \\|^2 ds\\, dt, $$\n",
        "\n",
        "where $x_s = (1-s) x_0 + s x_1$, and we also impose the condition $X_{\\theta}^{s, s}(x_s) = x_s$ within the architecture, namely we define\n",
        "\n",
        "$$X_{\\theta}^{t, s}(x) = (1 - (t-s) ) x + (t-s) f_{\\theta}(x, t, s).$$\n",
        "\n",
        "---\n",
        "\n",
        "### Sampling Using the Flow-Map Model\n",
        "\n",
        "For sampling, we can use either one-shot or few-shot sampling.\n",
        "\n",
        "* **One-shot**: Here, we just use $X^{1, 0}$, which takes a sample from $\\rho_0$ and maps it to a sample of $\\rho_1$.\n",
        "\n",
        "* **Few-shot**: We assume a partition of $[0, 1]$, e.g., $0=t_0\u003c t_1\u003c \\dots \u003c t_{n-1} \u003c t_n = 1$, and we factorize $X^{1,0}(x_0) = X^{1, t_{n-1}} \\circ X^{t_{n-1}, t_{n-2}} \\circ \\dots \\circ X^{t_1, 0} (x_0)$.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "[1] Flow Map Matching with Stochastic Interpolants: A Mathematical Framework for Consistency Models. Nicholas M. Boffi, Michael S. Albergo, and Eric Vanden-Eijnden."
      ]
    },
    {
      "metadata": {
        "id": "T7Kb-mlRip-R"
      },
      "cell_type": "markdown",
      "source": [
        "As usual we start by installing the `swirl-dynamics` library."
      ]
    },
    {
      "metadata": {
        "id": "HJ2_DwIv6N1v"
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/google-research/swirl-dynamics.git@main --quiet"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uJa_gfw46vVZ"
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "import os\n",
        "from clu import metric_writers\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import optax\n",
        "from orbax import checkpoint\n",
        "\n",
        "# Imports from swirl-dynamics codebase.\n",
        "from swirl_dynamics.lib.solvers import ode as ode_solvers\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import backbones\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import interpolants\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import losses\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import models as flow_models\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import trainers\n",
        "from swirl_dynamics.projects.distillation.flow_map_matching import models as flow_map_models\n",
        "from swirl_dynamics.projects.distillation.flow_map_matching import trainers as flow_map_trainers\n",
        "from swirl_dynamics.templates import callbacks\n",
        "from swirl_dynamics.templates import train\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "x31pmHPX7WtD"
      },
      "cell_type": "markdown",
      "source": [
        "We define the dataloader. Here we leverage the data in `tfds`, and we apply the appropiate transformations. Following the convention above we have that each batch will have three fields: \n",
        "- `x_0`: a sample from $\\rho_0$, i.e., a normal distribution,\n",
        "- `x_1`: a sample from $\\rho_1$, a sample from the MNIST dataset, and\n",
        "- `emb:label`: this is the class of the `x_1` sample, i.e., to which digits it corresponds."
      ]
    },
    {
      "metadata": {
        "id": "b7Ih3f_R7f9K"
      },
      "cell_type": "code",
      "source": [
        "def get_mnist_dataset(split: str, batch_size: int):\n",
        "  ds = tfds.load(\"mnist\", split=split)\n",
        "  ds = ds.map(\n",
        "      # Change field name from \"image\" to \"x\" (required by `DenoisingModel`)\n",
        "      # and normalize the value to [0, 1].\n",
        "      lambda x: {\"x_0\": tf.random.normal(shape=x[\"image\"].shape, mean=0.),\n",
        "                 \"x_1\": tf.cast(x[\"image\"], tf.float32) / 255.0,\n",
        "                 \"emb:label\": x[\"label\"]}\n",
        "  )\n",
        "  ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  ds = ds.as_numpy_iterator()\n",
        "  return ds"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "o2p-jAgg7jay"
      },
      "cell_type": "markdown",
      "source": [
        "We define the parameters for the neural architecture and the training hyperparameters. For simplicity we store them in a configDict and we will use them when necessary."
      ]
    },
    {
      "metadata": {
        "id": "uZkd46dJ7vpS"
      },
      "cell_type": "code",
      "source": [
        "import ml_collections\n",
        "\n",
        "config  = ml_collections.ConfigDict()\n",
        "\n",
        "config.initial_lr = 1e-6\n",
        "config.peak_lr  = 1e-3\n",
        "config.warmup_steps  = 10_000\n",
        "config.num_train_steps = 100_000\n",
        "config.num_train_steps_flow_map = 150_000\n",
        "config.end_lr = 1e-6\n",
        "config.beta1 = 0.999\n",
        "config.clip = 10.0\n",
        "config.save_interval_steps = 1000\n",
        "config.max_checkpoints_to_keep = 10\n",
        "\n",
        "config.metric_aggregation_steps = 100\n",
        "config.eval_every_steps = 5_000\n",
        "config.num_batches_per_eval = 1\n",
        "config.batch_size = 256\n",
        "config.batch_size_flow_map = 32\n",
        "\n",
        "config.out_channels = 1\n",
        "config.num_channels = (64, 128)\n",
        "config.downsample_ratio = (2, 2)\n",
        "config.num_blocks = 4\n",
        "config.noise_embed_dim = 128\n",
        "config.padding = \"SAME\"\n",
        "config.use_attention = True\n",
        "config.use_position_encoding = True\n",
        "config.num_heads = 8\n",
        "config.sigma_data = 0.31\n",
        "config.seed = 666\n",
        "config.ema_decay = 0.99\n",
        "\n",
        "\n",
        "config.input_shapes = ((1, 28, 28, 1), (1, 28, 28, 1))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "mIktY9vE8EPG"
      },
      "cell_type": "markdown",
      "source": [
        "## Defining the rectified flow (teacher) model."
      ]
    },
    {
      "metadata": {
        "id": "G7yGvB2b8Mx1"
      },
      "cell_type": "markdown",
      "source": [
        "Here we instantiate the model as a neural architecture, then we wrap it around using a `model` object, then we define the hyperparameters for training, the trained and we simply train the model. Given that this is already explained for the rectified flow [here](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/debiasing/rectified_flow/colab/demo_reflow.ipynb) and the stochastic interpolants [here](https://github.com/google-research/swirl-dynamics/tree/main/swirl_dynamics/projects/debiasing/stochastic_interpolants/colabs) we only provide a quick overview of the training pipeline.\n"
      ]
    },
    {
      "metadata": {
        "id": "HJQw4dBx84ep"
      },
      "cell_type": "markdown",
      "source": [
        "We define, instantiate and wrap the conditional rectified flow model using an stochastic interpolant."
      ]
    },
    {
      "metadata": {
        "id": "58RQu37G83_2"
      },
      "cell_type": "code",
      "source": [
        "flow_model = flow_models.RescaledUnet(\n",
        "    out_channels=1,\n",
        "    num_channels=config.num_channels,\n",
        "    downsample_ratio=config.downsample_ratio,\n",
        "    num_blocks=config.num_blocks,\n",
        "    noise_embed_dim=config.noise_embed_dim,\n",
        "    padding=config.padding,\n",
        "    use_attention=config.use_attention,\n",
        "    use_position_encoding=config.use_position_encoding,\n",
        "    num_heads=config.num_heads,\n",
        "    cond_embed_fn=backbones.MergeCategoricalEmbCond,\n",
        "    cond_embed_kwargs={\n",
        "        \"cond_key\": \"emb:label\",\n",
        "        \"num_classes\": 10,\n",
        "        \"features_embedding\": config.noise_embed_dim,\n",
        "        }\n",
        ")\n",
        "\n",
        "model = flow_models.ConditionalStochasticInterpolantModel(\n",
        "    input_shape= config.input_shapes[0][1:],\n",
        "    cond_shape = {\"emb:label\": ()},\n",
        "    conditioning_keys=(\"emb:label\",),\n",
        "    flow_model=flow_model,\n",
        "    interpolant=interpolants.RectifiedFlow(),  # Defines the type of stochastic interpolant.\n",
        "    loss_stochastic_interpolant=losses.velocity_loss,  # Defines the type of loss used for the training.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "OwF-TU4G-G6X"
      },
      "cell_type": "markdown",
      "source": [
        "Define the optimizer (with its corresponding schedule), and the trainer (including the checkpointer). We also define the workding directory to save the checkpoints."
      ]
    },
    {
      "metadata": {
        "id": "nmptkC928D2e"
      },
      "cell_type": "code",
      "source": [
        "schedule = optax.warmup_cosine_decay_schedule(\n",
        "    init_value=config.initial_lr,\n",
        "    peak_value=config.peak_lr,\n",
        "    warmup_steps=config.warmup_steps,\n",
        "    decay_steps=config.num_train_steps,\n",
        "    end_value=config.end_lr,\n",
        ")\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip(config.clip),\n",
        "    optax.adam(\n",
        "        learning_rate=schedule,\n",
        "        b1=config.beta1,\n",
        "    ),\n",
        ")\n",
        "trainer = trainers.DistributedStochasticInterpolantTrainer(\n",
        "    model=model,\n",
        "    rng=jax.random.PRNGKey(config.seed),\n",
        "    optimizer=optimizer,\n",
        "    ema_decay=config.ema_decay,\n",
        ")\n",
        "\n",
        "# Setting up checkpointing.\n",
        "ckpt_options = checkpoint.CheckpointManagerOptions(\n",
        "    save_interval_steps=config.save_interval_steps,\n",
        "    max_to_keep=config.max_checkpoints_to_keep,\n",
        ")\n",
        "\n",
        "workdir = os.path.join(os.getcwd(), \"velocity\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "bZCrCVvo-C7_"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the rectified flow model.\n",
        "\n",
        "Define the dataloders and train the rectified flow model.\n",
        "This step should take around 30 mins in a `TPU v6e`."
      ]
    },
    {
      "metadata": {
        "id": "S0FQdBje9-WI"
      },
      "cell_type": "code",
      "source": [
        "train_dataloader = get_mnist_dataset(split=\"train\", batch_size=config.batch_size)\n",
        "eval_dataloader = get_mnist_dataset(split=\"test\", batch_size=config.batch_size)\n",
        "\n",
        "# We avoid using the evaluation so the training runs a bit faster.\n",
        "train.run(\n",
        "    train_dataloader=train_dataloader,\n",
        "    trainer=trainer,\n",
        "    workdir=workdir,\n",
        "    total_train_steps=config.num_train_steps,\n",
        "    metric_aggregation_steps=config.metric_aggregation_steps,  # 30\n",
        "    num_batches_per_eval=config.num_batches_per_eval,\n",
        "    metric_writer=metric_writers.create_default_writer(workdir,\n",
        "                                                      asynchronous=False),\n",
        "    callbacks=(\n",
        "        # This callback shows a progress bar.\n",
        "        callbacks.TqdmProgressBar(\n",
        "            total_train_steps=config.num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        # This callback saves model checkpoint periodically.\n",
        "        callbacks.TrainStateCheckpoint(\n",
        "            base_dir=workdir,\n",
        "            options=ckpt_options,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qoARGGOo-XNd"
      },
      "cell_type": "markdown",
      "source": [
        "## Testing the trained model\n",
        "\n",
        "Here we consider the model above and we sample from it."
      ]
    },
    {
      "metadata": {
        "id": "pY-Y-LUoHJeA"
      },
      "cell_type": "markdown",
      "source": [
        "### Sampling from the trained model"
      ]
    },
    {
      "metadata": {
        "id": "mKQSTbgltier"
      },
      "cell_type": "code",
      "source": [
        "# Loads the weights.\n",
        "trained_flow_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
        "    f\"{workdir}/checkpoints\", step=None\n",
        ")\n",
        "\n",
        "# Extracts a batch, and gets the conditioning.\n",
        "num_samples: int = 32\n",
        "batch = next(iter(eval_dataloader))\n",
        "cond = {'emb:label': batch['emb:label'][:num_samples]}\n",
        "\n",
        "# Defines the dynamics.\n",
        "dynamics_fn = ode_solvers.nn_module_to_dynamics(\n",
        "      model.flow_model,\n",
        "      autonomous=False,\n",
        "      is_training=False,\n",
        "      cond=cond,\n",
        "  )\n",
        "\n",
        "num_sampling_steps = 128\n",
        "\n",
        "integrator = ode_solvers.RungeKutta4()\n",
        "integrate_fn = functools.partial(\n",
        "    integrator,\n",
        "    dynamics_fn,\n",
        "    tspan=jnp.arange(0.0, 1.0, 1.0 / num_sampling_steps),\n",
        "    params=trained_flow_state.model_variables,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "1xPNAmtZM8bR"
      },
      "cell_type": "markdown",
      "source": [
        "Here we solve the equation:\n",
        "\n",
        "$$\\dot{x_t} = v_{\\theta} (x_t, t),$$\n",
        "with inition condition given by $x_0 \\sim N(0, I_d)$., and where the terminal condition $x_1$ corresponds to our samples."
      ]
    },
    {
      "metadata": {
        "id": "Flg5TWON_wTx"
      },
      "cell_type": "code",
      "source": [
        "samples_flow = integrate_fn(batch[\"x_0\"][:num_samples])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "GzFKzWBSIrbX"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting the generated samples."
      ]
    },
    {
      "metadata": {
        "id": "mEb8Y2GMJAba"
      },
      "cell_type": "markdown",
      "source": [
        "Here we plot the samples. Consider that the network is quite small and it hasn't been exhaustively trained, so the quality of the samples can be further improved."
      ]
    },
    {
      "metadata": {
        "id": "1Ulg56fYI2Dy"
      },
      "cell_type": "code",
      "source": [
        "num_cols = 6\n",
        "plt.figure(figsize=(num_cols*6, 5))\n",
        "for i in range(1,num_cols+1):\n",
        "    plt.subplot(1, num_cols, i)\n",
        "    plt.imshow(samples_flow[-1, i-1, :, :, 0])\n",
        "    plt.yticks(ticks=[], labels=[])\n",
        "    plt.xticks(ticks=[], labels=[])\n",
        "    plt.title(f\"Label: {cond['emb:label'][i-1]}\", fontsize=16)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "o1Rf1aq8JPEE"
      },
      "cell_type": "markdown",
      "source": [
        "## Setting up the flow-map model for distillation.\n",
        "\n",
        "Similarly to above we construct the flow-map model. In this case we use a very similar model although, with some differences to be adapted to the"
      ]
    },
    {
      "metadata": {
        "id": "XdMShwlsJSax"
      },
      "cell_type": "code",
      "source": [
        "flow_map_nn_model = flow_map_models.RescaledFlowMapUNet(\n",
        "    time_rescale=1.0,\n",
        "    out_channels=1,\n",
        "    num_channels=config.num_channels,\n",
        "    downsample_ratio=config.downsample_ratio,\n",
        "    num_blocks=config.num_blocks,\n",
        "    noise_embed_dim=config.noise_embed_dim,\n",
        "    padding=config.padding,\n",
        "    use_attention=config.use_attention,\n",
        "    use_position_encoding=config.use_position_encoding,\n",
        "    num_heads=32,\n",
        "    frequency_scaling=\"exponential\",\n",
        "    cond_embed_fn=backbones.MergeCategoricalEmbCond,\n",
        "    cond_embed_kwargs={\n",
        "        \"cond_key\": \"emb:label\",\n",
        "        \"num_classes\": 10,\n",
        "        \"features_embedding\": config.noise_embed_dim,\n",
        "        }\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Sig3V47gJz5K"
      },
      "cell_type": "markdown",
      "source": [
        "Here we define the wrapper for the conditional flow map model. Given that this model is supposed to perform distillation it also requires the teacher model (flow_model) and its weights so it can be evaluated."
      ]
    },
    {
      "metadata": {
        "id": "9DOTfMNHJzWy"
      },
      "cell_type": "code",
      "source": [
        "flow_map_model = flow_map_models.ConditionalLagrangianFlowMapModel(\n",
        "    input_shape=config.input_shapes[0][1:],\n",
        "    cond_shape={'emb:label': ()}, # This is a scalar.\n",
        "    flow_model=flow_model,  # Teacher model.\n",
        "    flow_map_model=flow_map_nn_model,  # Student model.\n",
        "    params_flow=trained_flow_state.model_variables[\"params\"], # Parameteres of the teacher model\n",
        "    interpolant=interpolants.RectifiedFlow(),  # Defines the type of stochastic interpolant.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "-vBN_pcNEE1N"
      },
      "cell_type": "code",
      "source": [
        "# Defining experiments through the config file.\n",
        "schedule = optax.warmup_cosine_decay_schedule(\n",
        "    init_value=1e-6,\n",
        "    peak_value=2e-4,\n",
        "    warmup_steps=5_000,\n",
        "    decay_steps=config.num_train_steps_flow_map,\n",
        "    end_value=1e-6,\n",
        ")\n",
        "\n",
        "optimizer_flow_map = optax.chain(\n",
        "    optax.clip(config.clip),\n",
        "    optax.adam(\n",
        "        learning_rate=schedule,\n",
        "        b1=config.beta1,\n",
        "    ),\n",
        ")\n",
        "\n",
        "# If you have multidevice host, it will automatically distribute the training.\n",
        "flow_map_trainer = flow_map_trainers.DistributedLagrangianFlowMapTrainer(\n",
        "    model=flow_map_model,\n",
        "    rng=jax.random.PRNGKey(config.seed),\n",
        "    optimizer=optimizer_flow_map,\n",
        "    ema_decay=config.ema_decay,\n",
        ")\n",
        "\n",
        "# Setting up checkpointing.\n",
        "ckpt_options_flow_map = checkpoint.CheckpointManagerOptions(\n",
        "    save_interval_steps=config.save_interval_steps,\n",
        "    max_to_keep=config.max_checkpoints_to_keep,\n",
        ")\n",
        "\n",
        "workdir_flow = os.path.join(os.getcwd(), \"flow_map\")\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5DBI1LEoTF4D"
      },
      "cell_type": "markdown",
      "source": [
        "Use this command to erase any previous checkpoint that you may already have."
      ]
    },
    {
      "metadata": {
        "id": "JggryCiMS49D"
      },
      "cell_type": "code",
      "source": [
        "!rm -R -f /content/flow_map"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "M9IGVXmvIpdO"
      },
      "cell_type": "markdown",
      "source": [
        "Running the traning loop. In this case most we will just load the pre-trained weights from the working directory.\n",
        "\n",
        "This should take roughly 2 hours in a `TPU v6e`."
      ]
    },
    {
      "metadata": {
        "id": "PretbUmUANCE"
      },
      "cell_type": "code",
      "source": [
        "# Here we use a smaller batch size as the computational burden is higher.\n",
        "train_dataloader = get_mnist_dataset(split=\"train\",\n",
        "                                     batch_size=config.batch_size_flow_map)\n",
        "\n",
        "# Here we don't run evaluation to render the training a bit faster.\n",
        "train.run(\n",
        "    train_dataloader=train_dataloader,\n",
        "    trainer=flow_map_trainer,\n",
        "    workdir=workdir_flow,\n",
        "    total_train_steps=config.num_train_steps_flow_map,\n",
        "    metric_aggregation_steps=config.metric_aggregation_steps,\n",
        "    num_batches_per_eval=config.num_batches_per_eval,\n",
        "    metric_writer=metric_writers.create_default_writer(workdir_flow,\n",
        "                                                       asynchronous=False),\n",
        "    callbacks=(\n",
        "        callbacks.TqdmProgressBar(\n",
        "            total_train_steps=config.num_train_steps_flow_map,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        # This callback saves model checkpoint periodically.\n",
        "        callbacks.TrainStateCheckpoint(\n",
        "            base_dir=workdir_flow,\n",
        "            options=ckpt_options_flow_map,\n",
        "        ),\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "p3MsoxEhOWQg"
      },
      "cell_type": "markdown",
      "source": [
        "## Sampling using the distilled model."
      ]
    },
    {
      "metadata": {
        "id": "DysS4LBmQ4Uo"
      },
      "cell_type": "code",
      "source": [
        "trained_flow_map_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
        "    f\"{workdir_flow}/checkpoints\", step=None\n",
        ")\n",
        "inference_fn = flow_map_model.inference_fn(trained_flow_map_state.model_variables, flow_map_model.flow_map_model )"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "B-HMDJeiQ8lc"
      },
      "cell_type": "code",
      "source": [
        "# We test the one step generation starting from noise.\n",
        "samples_flow_map = inference_fn(batch['x_0'][:num_samples],\n",
        "                   jnp.ones((32,)),\n",
        "                   jnp.zeros((32,)),\n",
        "                   {\"emb:label\": batch[\"emb:label\"][:num_samples]},)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "qPtQAPX6RJx1"
      },
      "cell_type": "markdown",
      "source": [
        "We plot the samples (using the same labels an initial noise as the ones solved using the ODE)."
      ]
    },
    {
      "metadata": {
        "id": "KRpWwxKS6jWi"
      },
      "cell_type": "code",
      "source": [
        "samples_flow_map.shape"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "NMLV9KB5RISt"
      },
      "cell_type": "code",
      "source": [
        "num_cols = 6\n",
        "plt.figure(figsize=(num_cols * 6, 5))\n",
        "for i in range(1,num_cols+1):\n",
        "    plt.subplot(1, num_cols, i)\n",
        "    plt.imshow(samples_flow_map[i-1, :, :, 0])\n",
        "    plt.yticks(ticks=[], labels=[])\n",
        "    plt.xticks(ticks=[], labels=[])\n",
        "    plt.title(f\"Label: {cond['emb:label'][i-1]}\", fontsize=16)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "YdePXZ8MRiAy"
      },
      "cell_type": "markdown",
      "source": [
        "### Few-shots generation.\n",
        "\n",
        "Here we check the quality of the generation as we increase the number of applications of the network. Due to the underlying Markovian property of the ODE, we have that $X^{1, 0}(x_0)$ can be writen as\n",
        "\n",
        "$$ X^{1, 0}(x_0) = X^{1, t_{n-1}} \\circ X^{t_{n-1}, t_{n-2}}  \\dots  \\circ X^{t_1, 0}(x_0),$$\n",
        "\n",
        "for a given partition of $[0,1]$ into intervals $\\{ [t_{n}, t_{n+1}]\\}_{i=0}^{n-1}$, where $0 = t_0 \u003c t_{1} \u003c t_{2} ... \u003c t_{n-1} \u003c t_{n} = 1.$\n",
        "\n",
        "Following the main paper, usually having more partitions helps with the quality of the generated samples."
      ]
    },
    {
      "metadata": {
        "id": "Lm3Y9vhnRgp_"
      },
      "cell_type": "code",
      "source": [
        "flow_map_samples_dict = {}\n",
        "cond_test = {\"emb:label\": batch[\"emb:label\"][:num_samples]}\n",
        "def body_for_loop(i, x, cond, delta_t):\n",
        "  return inference_fn(x,\n",
        "                      delta_t * (i + 1) * jnp.ones((x.shape[0],)),\n",
        "                      delta_t * i * jnp.ones((x.shape[0],)),\n",
        "                      cond\n",
        "                      )\n",
        "\n",
        "number_of_eval_steps: tuple[int, ...] = (1, 2, 4, 8, 16, 32)\n",
        "\n",
        "for num_steps in number_of_eval_steps:\n",
        "  delta_t = 1./ num_steps\n",
        "  body_for_loop = functools.partial(body_for_loop,\n",
        "                                    delta_t=delta_t,\n",
        "                                    cond=cond_test)\n",
        "  samples = jax.lax.fori_loop(\n",
        "      0, num_steps, body_for_loop, batch[\"x_0\"][:num_samples]\n",
        "  )\n",
        "  flow_map_samples_dict[f\"step_{num_steps}\"] = samples\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2_8GDCwMR95a"
      },
      "cell_type": "markdown",
      "source": [
        "### Utility function to show the samples in a grid."
      ]
    },
    {
      "metadata": {
        "id": "mNjXRKRAR1f-"
      },
      "cell_type": "code",
      "source": [
        "def plot_samples_grid(eval_samples: dict, cond: dict, num_cols:int =12 )-\u003e None:\n",
        "  \"\"\"Plots the samples in the eval_samples dict by label.\"\"\"\n",
        "  num_rows = len(eval_samples)\n",
        "  plt.figure(figsize=(num_cols*6, 5*num_rows))\n",
        "  for i in range(1,num_cols+1):\n",
        "    for j, (key, sample) in enumerate(eval_samples.items()):\n",
        "      plt.subplot(num_rows, num_cols, i + j*num_cols)\n",
        "      plt.imshow(sample[i-1, :, :, 0])\n",
        "      plt.yticks(ticks=[], labels=[])\n",
        "      plt.xticks(ticks=[], labels=[])\n",
        "      if i == 1:\n",
        "        plt.ylabel(key, fontsize=16)\n",
        "      if j== 0:\n",
        "        plt.title(f\"label: {cond['emb:label'][i-1]}\", fontsize=16)\n",
        "\n",
        "  plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "HH6SaBu9SFri"
      },
      "cell_type": "markdown",
      "source": [
        "### Plotting the samples on a grid, with different labels and different number of applications."
      ]
    },
    {
      "metadata": {
        "id": "Fti6ZxnPR7NA"
      },
      "cell_type": "code",
      "source": [
        "plot_samples_grid(flow_map_samples_dict,\n",
        "                  cond_test,\n",
        "                  num_cols=16)"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "11ARNc94fSpNInJiZc04JxMlZ08-dZ2Wc",
          "timestamp": 1752092387493
        },
        {
          "file_id": "1rxoKcc3xfBTAUbhWxP2NsG6IF_eM8bAq",
          "timestamp": 1752092195810
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
