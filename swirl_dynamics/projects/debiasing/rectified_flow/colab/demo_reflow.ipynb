{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htEEY6RvQ8Tp"
      },
      "source": [
        "# Rectified Flow example for Debiasing PDE solutions.\n",
        "\n",
        "Here we consider the Kuramoto-Sivashinsky equation:\n",
        "$$\\partial_t u = u \\partial_x u + \\partial_{xx}u - \\partial_{xxxx}u,$$\n",
        "a prime example of a chaotic PDE.\n",
        "\n",
        "Here considered data steming from two different solvers. For the ground-truth, or reference, dataset we consider data generated by solving the KS equation using a pseudo-spectal solver with a fine mesh. For the biased data we consider data generated using a finite volumes (FV) solver using a much coarser mesh. The data was generated using [jax-cfd](https://github.com/google/jax-cfd) using the same randomly generated initial conditions and with the same warm-up period.\n",
        "\n",
        "Due to the difference in the resolution and extra dissipativity incurred by the FV discretization, the trajectories computed with FV are \"biased\" compared to the ones stemming from the pseudo-spectral methods. This can be readily observed when inspecting histograms of the snapshots, in which we observe a different distributions when considering values of the numerical approximation to the same PDE but using diffrent numerical methods.\n",
        "\n",
        "In this colab we seek to show how to use rectified flows to address the statistical errors of the coarse, FV based solution to match the statitics of the fine, spectrally accurate solutions. This is what we refer to as *debiasing*.\n",
        "\n",
        "This colab uses the same methodology proposed in [Wan et al](https://arxiv.org/abs/2412.08079) but using a simpler dataset: the dimension is much lower, we only consider one field, there is no climatology, and we suppose that underlying distribution is fixed. In what follows, we seek to walk the reader on how the full pipeline is implemented, including how to define dataloaders, while leveraging the [swirl-dynamics](https://github.com/google-research/swirl-dynamics) library to perform most of the heavy lifting for training and inference.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcRQQ_bZTzaf"
      },
      "source": [
        "### Downloading dependencies.\n",
        "\n",
        "We use the `swirl-dynamics` library for most of the heavy lifting, so we install it using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFKh8QMPRKuv"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-research/swirl-dynamics.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suLdZxuhE4qv"
      },
      "source": [
        "We also import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjF7tVIERgGH"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "# jax.config.update(\"jax_enable_x64\", True)\n",
        "import jax.numpy as jnp\n",
        "from clu import metric_writers\n",
        "\n",
        "import abc\n",
        "from collections.abc import Callable, Mapping, Sequence\n",
        "import types\n",
        "from typing import Any, Literal, SupportsIndex\n",
        "\n",
        "import grain.python as pygrain\n",
        "import ml_collections\n",
        "import h5py\n",
        "import optax\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import flax.linen as nn\n",
        "import flax\n",
        "import functools\n",
        "\n",
        "from orbax import checkpoint\n",
        "\n",
        "from swirl_dynamics.projects.debiasing.rectified_flow import pygrain_transforms as transforms\n",
        "from swirl_dynamics.projects.debiasing.rectified_flow import models\n",
        "from swirl_dynamics.projects.debiasing.rectified_flow import trainers\n",
        "from swirl_dynamics.templates import callbacks\n",
        "from swirl_dynamics.templates import train\n",
        "from swirl_dynamics.lib import layers\n",
        "from swirl_dynamics.lib.diffusion import unets\n",
        "\n",
        "\n",
        "# Defines shortened types\n",
        "ConfigDict = ml_collections.config_dict.config_dict.ConfigDict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn6RrtNTU-ew"
      },
      "source": [
        "### Define Hyper-Parameters\n",
        "\n",
        "For simplicity we define the parameters inside a `ConfigDict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLz7hEEGUrE2"
      },
      "outputs": [],
      "source": [
        "config = ml_collections.ConfigDict()\n",
        "\n",
        "# Parameters of the trainer. If multidevice host, you can turn this to True.\n",
        "# The caveat is that the batch_size needs to be mutiple of the num_devices.\n",
        "config.is_distributed = False\n",
        "config.seed = 37\n",
        "\n",
        "# Optimizer parameters.\n",
        "config.initial_lr = 1e-7\n",
        "config.peak_lr = 5e-4\n",
        "config.end_lr = 1e-7\n",
        "config.warmup_steps = 5_000\n",
        "config.beta1 = 0.9\n",
        "config.max_norm = 0.6\n",
        "config.ema_decay = 0.99\n",
        "\n",
        "# Training/Evaluation parameters.\n",
        "config.num_train_steps = 25_000\n",
        "config.num_batches_per_eval = 8\n",
        "config.metric_aggregation_steps = 100\n",
        "config.eval_every_steps = 500\n",
        "\n",
        "# Checkpointing parameters.\n",
        "config.save_interval_steps = 1_000\n",
        "config.max_checkpoints_to_keep = 10\n",
        "\n",
        "\n",
        "# Batch parameters.\n",
        "config.batch_size = 64\n",
        "config.eval_batch_size = 8\n",
        "\n",
        "# Data parameters. Here we have the input/output shape.\n",
        "config.input_shapes = ((1, 48, 1), (1, 48, 1))\n",
        "config.normalize = False  # Wheter to normalize the data or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrXypPLGUGXg"
      },
      "source": [
        "### Downloading the data.\n",
        "\n",
        "The data was generated using [jax-cfd](https://github.com/google/jax-cfd), and uploaded to a Google Cloud bucket. The file contains both high- and low-resolution datasets.\n",
        "\n",
        "We use gsutil for downloading the data. If you are running this notebook in colab, it should be already installed, otherwise, you can follow these [instructions](https://cloud.google.com/storage/docs/gsutil_install) to install it in your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQCJM4L0RXje"
      },
      "outputs": [],
      "source": [
        "!gsutil cp gs://gresearch/swirl_dynamics/downscaling/KS_finite_volumes_vs_pseudo_spectral.hdf5 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa5rMtlFRdGI"
      },
      "outputs": [],
      "source": [
        "file_name = 'KS_finite_volumes_vs_pseudo_spectral.hdf5'\n",
        "\n",
        "with h5py.File(file_name,'r+') as file:\n",
        "    u_lr    = file['u_fd'][()] # Trajectories with finite volumes.\n",
        "    u_hr    = file['u_sp'][()] # Trajectories with pseudo spectral methods.\n",
        "    x    = file['x'][()] # Grid in which the trajectories are computed.\n",
        "    t    = file['t'][()] # Time stamps for the trajectories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7oDj-FbRsnZ"
      },
      "outputs": [],
      "source": [
        "# Plotting the low-res data.\n",
        "# We choose which trajectory we want to plot.\n",
        "plot_idx = 1\n",
        "# Spatial downsampling factor.\n",
        "ds_x = 4\n",
        "\n",
        "# Define domain in time and space.\n",
        "x_ = jnp.concatenate([x, jnp.array(x[-1] + x[1] - x[0]).reshape((-1,))])[::ds_x]\n",
        "print(f\"Shape of the spatial domain: {x_.shape}\")\n",
        "\n",
        "# Plots the low-resolution data.\n",
        "fig = plt.figure(figsize=(14, 4))\n",
        "plt.imshow(u_lr[plot_idx, :, :].T, extent=[t[0],t[-1],x_[-1],x_[0]], aspect=5)\n",
        "plt.xlabel(\"time\")\n",
        "plt.ylabel(\"x\")\n",
        "plt.show()\n",
        "\n",
        "# Plots the low-resolution data.\n",
        "fig = plt.figure(figsize=(14, 4))\n",
        "plt.imshow(u_hr[plot_idx, :, :].T, extent=[t[0],t[-1],x_[-1],x_[0]], aspect=5)\n",
        "plt.xlabel(\"time\")\n",
        "plt.ylabel(\"x\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XdXMh2zSW4v"
      },
      "outputs": [],
      "source": [
        "u_lr_hf = u_hr[:, :, ::ds_x]\n",
        "x_lr_hf = x_[::ds_x]\n",
        "u_lr_lf = u_lr\n",
        "\n",
        "print(f\"Shape of the low-resolution high-fidelity data {u_lr_hf.shape}\")\n",
        "print(f\"Shape of the low-resolution grid {x_lr_hf.shape}\")\n",
        "print(f\"Shape of the low-resolution low-fidelity data {u_lr_lf.shape}\")"
      ]
    },
    {
      "metadata": {
        "id": "2oJIYZLrBmom"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the dataloaders.\n",
        "\n",
        "We leverage [pygrain](https://github.com/google/grain) to feed the data to the models during training. As such we need to define a source, which allows random access through an index sampling to the data, and the dataloader itself, which allows us to easily define transformations, such as normalization, concatenatation, change of keys, among others. \n",
        "\n",
        "We will use the simple tensorized coupling during training, i.e., the pairs are sampled following\n",
        "$$(x_0, x_1) \\sim \\mu_0 \\otimes \\mu_1,$$\n",
        "where $\\mu_0$ and $\\mu_1$ are the distribution of the input/output data respectively. Therefore, we will define dataloaders for each dataset, which will be sampled independently.\n"
      ]
    },
    {
      "metadata": {
        "id": "RxfYSupZBmom"
      },
      "cell_type": "markdown",
      "source": [
        "Here we define the `DataSource`. As the data is fairly small, we will read it from RAM, by returning a slice of a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dsJQts1TVDD"
      },
      "outputs": [],
      "source": [
        "class SourceInMemoryNumpy(pygrain.RandomAccessDataSource):\n",
        "  \"\"\"Source class for Numpy array.\n",
        "\n",
        "  Attributes:\n",
        "    source: Numpy array with the data.\n",
        "    normalize_stats: Dictionary containing the mean and std statistics of the\n",
        "      data.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      dataset: np.ndarray,\n",
        "  ):\n",
        "    \"\"\"Build the source of pre-computed trajectories stored in a numpy array.\n",
        "\n",
        "    Args:\n",
        "      dataset: Dataset in numpy format.\n",
        "\n",
        "    Returns:\n",
        "      loader, stats: Tuple of dataloader and dictionary containing mean and std.\n",
        "    \"\"\"\n",
        "\n",
        "    self.source = dataset\n",
        "\n",
        "    # Computes the metrics and store them in a dictionary.\n",
        "    mean = np.mean(self.source, axis=0)\n",
        "    std = np.std(self.source, axis=0)\n",
        "    self.normalize_stats = {\"mean\": mean, \"std\": std}\n",
        "\n",
        "  def __len__(self) -\u003e int:\n",
        "    \"\"\"Returns the number of samples in the source.\"\"\"\n",
        "    return self.source.shape[0]\n",
        "\n",
        "  def __getitem__(self, record_key: SupportsIndex) -\u003e Mapping[str, np.ndarray]:\n",
        "    \"\"\"Returns the data record for a given record key.\n",
        "\n",
        "    Args:\n",
        "      record_key: The index to the position in the array\n",
        "\n",
        "    Returns:\n",
        "      A dictionay with the slice of the underlying array and key \"u\".\n",
        "    \"\"\"\n",
        "    idx = record_key.__index__()\n",
        "\n",
        "    if idx \u003e= self.__len__():\n",
        "      raise IndexError(\"Index out of range.\")\n",
        "    # here we return a dictionary with \"u\"\n",
        "    return {\"u\": self.source[idx]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI1hO2VbXZb-"
      },
      "outputs": [],
      "source": [
        "def create_loader_from_ndarray(\n",
        "    dataset: np.ndarray,\n",
        "    batch_size: int,\n",
        "    seed: int = 999,\n",
        "    shuffle: bool = True,\n",
        "    normalize: bool = True,\n",
        "    normalize_stats: dict[str, np.ndarray] | None = None,\n",
        "    drop_remainder: bool = True,\n",
        "    worker_count: int = 0,\n",
        "    output_name: Literal['x_0', 'x_1'] = 'x_0',\n",
        "    num_epochs: int | None = None,\n",
        ") -\u003e tuple[pygrain.DataLoader, dict[str, np.ndarray] | None]:\n",
        "  \"\"\"Load pre-computed trajectories dumped to hdf5 file.\n",
        "\n",
        "  Args:\n",
        "    dataset: Absolute path to dataset file.\n",
        "    batch_size: Batch size returned by dataloader. If set to -1, use entire\n",
        "      dataset size as batch_size.\n",
        "    seed: Random seed to be used in data sampling.\n",
        "    shuffle: Whether to randomly shuffle the data.\n",
        "    normalize: Flag for adding data normalization (subtact mean divide by std.).\n",
        "    normalize_stats: Dictionary with mean and std stats to avoid recomputing, or\n",
        "      if they need to be computed from a different dataset.\n",
        "    output_name: Name of the output feature in the dictionary.\n",
        "    drop_remainder: Flag for dropping the last batch if it is not complete.\n",
        "    worker_count: Number of workers to use in the dataloader.\n",
        "    output_name: Name of the field in the output, here we only consider either\n",
        "      the initial or terminal value of the ODE, i.e., \"x_0\" or \"x_1\".\n",
        "    num_epochs: Number of epochs, by default it will never stop.\n",
        "\n",
        "  Returns:\n",
        "    loader, stats (optional): Tuple of dataloader and dictionary containing\n",
        "                              mean and std stats (if normalize=True, else dict\n",
        "                              contains NoneType values).\n",
        "  \"\"\"\n",
        "\n",
        "  # Creates the source file, which is a random access file wrapping a numpy\n",
        "  # array\n",
        "  source = SourceInMemoryNumpy(dataset)\n",
        "\n",
        "  if normalize_stats is None:\n",
        "    normalize_stats = source.normalize_stats\n",
        "\n",
        "  if \"mean\" not in normalize_stats or \"std\" not in normalize_stats:\n",
        "    raise ValueError(\n",
        "        \"The normalize_stats dictionary should contain keys 'mean' and 'std'.\"\n",
        "    )\n",
        "\n",
        "  transformations = []\n",
        "  if normalize:\n",
        "    transformations.append(\n",
        "        transforms.Standardize(\n",
        "            input_fields=[\"u\",],\n",
        "            mean={\"u\": normalize_stats[\"mean\"]},\n",
        "            std={\"u\": normalize_stats[\"std\"]},\n",
        "        )\n",
        "    )\n",
        "\n",
        "  if output_name is not None and output_name != \"u\":\n",
        "    if not isinstance(output_name, str):\n",
        "      raise ValueError(\n",
        "          \"The output_name should be a string, but it is a \",\n",
        "          type(output_name),\n",
        "      )\n",
        "    transformations.append(\n",
        "        transforms.SelectAs(\n",
        "            select_features=[\"u\",],\n",
        "            as_features=[output_name,]\n",
        "        )\n",
        "    )\n",
        "\n",
        "  loader = pygrain.load(\n",
        "      source=source,\n",
        "      num_epochs=num_epochs,\n",
        "      shuffle=shuffle,\n",
        "      seed=seed,\n",
        "      shard_options=pygrain.ShardByJaxProcess(drop_remainder=True),\n",
        "      transformations=transformations,\n",
        "      batch_size=batch_size,\n",
        "      drop_remainder=drop_remainder,\n",
        "      worker_count=worker_count,\n",
        "  )\n",
        "  return loader, normalize_stats"
      ]
    },
    {
      "metadata": {
        "id": "tni5LU34Bmom"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we leverage each individual dataset which samples either from $\\mu_0$ of $\\mu_1$ to build the dataloader that samples from $\\mu_0 \\otimes \\mu_1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2DEfLchZ-G1"
      },
      "outputs": [],
      "source": [
        "class UnpairedDataLoaderNumpy:\n",
        "  \"\"\"Unpaired dataloader for loading samples from two distributions.\n",
        "\n",
        "  Attributes:\n",
        "    loader_a: PyGrain dataloader for the input data.\n",
        "    loader_b: PyGrain dataloader for the output data.\n",
        "    normalize_stats_a: Dictionary with the statisitcs, mean and std of the\n",
        "      input data.\n",
        "    normalize_stats_b: Dictionary with the statisitcs, mean and std of the\n",
        "      output data.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      dataset_a: np.ndarray,\n",
        "      dataset_b: np.ndarray,\n",
        "      batch_size: int,\n",
        "      seed: int = 37,\n",
        "      normalize: bool = False,\n",
        "      normalize_stats_a: dict[str, np.ndarray] | None = None,\n",
        "      normalize_stats_b: dict[str, np.ndarray] | None = None,\n",
        "      drop_remainder: bool = True,\n",
        "      worker_count: int = 0,\n",
        "  ):\n",
        "\n",
        "    loader, normalize_stats_a = create_loader_from_ndarray(\n",
        "        dataset=dataset_a,\n",
        "        batch_size=batch_size,\n",
        "        seed=seed,\n",
        "        normalize=normalize,\n",
        "        normalize_stats=normalize_stats_a,\n",
        "        output_name=\"x_0\",\n",
        "        drop_remainder=drop_remainder,\n",
        "        worker_count=worker_count,\n",
        "    )\n",
        "\n",
        "    self.loader_a = iter(loader)\n",
        "\n",
        "    loader, normalize_stats_b = create_loader_from_ndarray(\n",
        "        dataset=dataset_b,\n",
        "        batch_size=batch_size,\n",
        "        seed=seed+1,\n",
        "        normalize=normalize,\n",
        "        normalize_stats=normalize_stats_b,\n",
        "        output_name=\"x_1\",\n",
        "        drop_remainder=drop_remainder,\n",
        "        worker_count=worker_count,\n",
        "    )\n",
        "    self.loader_b = iter(loader)\n",
        "\n",
        "    self.normalize_stats_a = normalize_stats_a\n",
        "    self.normalize_stats_b = normalize_stats_b\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  def __next__(self) -\u003e dict[str, np.ndarray]:\n",
        "    \"\"\"Defines the next function for the iterator.\"\"\"\n",
        "\n",
        "    b = next(self.loader_b)\n",
        "    a = next(self.loader_a)\n",
        "\n",
        "    # Returns dictionary with keys \"x_0\" and \"x_1\".\n",
        "    return {**a, **b}"
      ]
    },
    {
      "metadata": {
        "id": "lUkHNrkcBmom"
      },
      "cell_type": "markdown",
      "source": [
        "We instantiate both the training and evaluation dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeuSQyqWbFeS"
      },
      "outputs": [],
      "source": [
        "# We consider the last 16 trajectories as evaluation ones.\n",
        "num_test_trajs = 16\n",
        "\n",
        "train_dataloader = UnpairedDataLoaderNumpy(\n",
        "    dataset_a=u_lr_lf[:-num_test_trajs].reshape((-1, u_lr_lf.shape[-1], 1)),\n",
        "    dataset_b=u_lr_hf[:-num_test_trajs].reshape((-1, u_lr_lf.shape[-1], 1)),\n",
        "    batch_size=config.batch_size,\n",
        "    normalize=config.normalize,\n",
        "  )\n",
        "\n",
        "eval_dataloader = UnpairedDataLoaderNumpy(\n",
        "    dataset_a=u_lr_lf[-num_test_trajs:].reshape((-1, u_lr_lf.shape[-1], 1)),\n",
        "    dataset_b=u_lr_hf[-num_test_trajs:].reshape((-1, u_lr_lf.shape[-1], 1)),\n",
        "    # We use the same stats as the train data.\n",
        "    normalize_stats_a=train_dataloader.normalize_stats_a,\n",
        "    normalize_stats_b=train_dataloader.normalize_stats_b,\n",
        "    batch_size=config.eval_batch_size,\n",
        "    normalize=config.normalize,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9Vv5CKoddrV"
      },
      "source": [
        "We quickly test the batches produced by the dataloader, in particular is should contain the both the input and output samples, which are indexed by the keys `x_0` and `x_1`, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQGltITDZgoA"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(batch.keys())\n",
        "assert batch[\"x_0\"].shape[0] == config.batch_size\n",
        "print(batch[\"x_0\"].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyDc4ykUY0S"
      },
      "source": [
        "### Defining the rectified flow optimizers.\n",
        "\n",
        "Here we define the learning rate schedule, for simplicity we use a linear ramp-up followed with a cosine decay schedule. This can be further tweaked but empirically, this has shown to provide reasonable resutls for this type of problems.\n",
        "\n",
        "For the optimizer we use the Adam optimizer, and we also add a clipping mechanism to help avoid instabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr9nX8QvUfrl"
      },
      "outputs": [],
      "source": [
        "# Defining experiments through the config file.\n",
        "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "    init_value=config.initial_lr,\n",
        "    peak_value=config.peak_lr,\n",
        "    warmup_steps=config.warmup_steps,\n",
        "    decay_steps=config.num_train_steps,\n",
        "    end_value=config.end_lr,\n",
        ")\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.clip_by_global_norm(config.max_norm),\n",
        "    optax.adam(\n",
        "        learning_rate=lr_schedule,\n",
        "        b1=config.beta1,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3HHc7egnxWl"
      },
      "source": [
        "## Defining the model\n",
        "\n",
        "In this case the model is a fully convolutional model, with several ResNet blocks with a Fourier embedding for the time.\n",
        "\n",
        "Here this model paremetrized the velocity vector field in the Rectiflied formulation, which is given by solving the following ODE:\n",
        "$$\\dot{x} = v_{\\theta}(x, t),$$\n",
        "in this case, the model defined below parametrized $v_{\\theta}(x, t)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocXSM5oEnwj7"
      },
      "outputs": [],
      "source": [
        "Array = jax.Array\n",
        "\n",
        "class ConvModel(nn.Module):\n",
        "  out_channels: int = 1\n",
        "  num_channels: tuple[int,...] = (32, 32, 32)\n",
        "  num_blocks: int = 4\n",
        "  noise_embed_dim: int = 32\n",
        "  kernel_dim: tuple[int] = (5,)\n",
        "  padding: str = \"CIRCULAR\"\n",
        "  precision: jax.lax.Precision | None = 'fastest'\n",
        "  dtype: jnp.dtype = jnp.float32\n",
        "  param_dtype: jnp.dtype = jnp.float32\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(\n",
        "      self,\n",
        "      x: Array,\n",
        "      sigma: Array,\n",
        "      cond: dict[str, Array] | None = None,\n",
        "      *,\n",
        "      is_training: bool,\n",
        "  ) -\u003e Array:\n",
        "\n",
        "    if sigma.ndim \u003c 1:\n",
        "      sigma = jnp.broadcast_to(sigma, (x.shape[0],))\n",
        "\n",
        "    if sigma.ndim != 1 or x.shape[0] != sigma.shape[0]:\n",
        "      raise ValueError(\n",
        "          \"sigma must be 1D and have the same leading (batch) dimension as x\"\n",
        "          f\" ({x.shape[0]})!\"\n",
        "      )\n",
        "\n",
        "    emb = unets.FourierEmbedding(dims=self.noise_embed_dim)(sigma)\n",
        "\n",
        "    h = layers.ConvLayer(\n",
        "          features=128,\n",
        "          kernel_size=self.kernel_dim,\n",
        "          padding=self.padding,\n",
        "          kernel_init=unets.default_init(1.0),\n",
        "          precision=self.precision,\n",
        "          dtype=self.dtype,\n",
        "          param_dtype=self.param_dtype,\n",
        "          name=\"conv_in\",\n",
        "      )(x)\n",
        "\n",
        "    for i, channel in enumerate(self.num_channels):\n",
        "      for j in range(self.num_blocks):\n",
        "        h = unets.ConvBlock(\n",
        "                  out_channels=channel,\n",
        "                  kernel_size=self.kernel_dim,\n",
        "                  padding=self.padding,\n",
        "                  # dropout=self.dropout_rate,\n",
        "                  precision=self.precision,\n",
        "                  dtype=self.dtype,\n",
        "                  param_dtype=self.param_dtype,\n",
        "                  name=f\"block_number{i}_size_{channel}.num_subblock_{j}.\",\n",
        "              )(h, emb, is_training=is_training)\n",
        "\n",
        "    h = layers.ConvLayer(\n",
        "          features=self.out_channels,\n",
        "          kernel_size=self.kernel_dim,\n",
        "          padding=self.padding,\n",
        "          kernel_init=unets.default_init(1.0),\n",
        "          precision=self.precision,\n",
        "          dtype=self.dtype,\n",
        "          param_dtype=self.param_dtype,\n",
        "          name=\"conv_out\",\n",
        "      )(h)\n",
        "\n",
        "    return h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT317R1Uifyy"
      },
      "source": [
        "### Instantiating the model.\n",
        "\n",
        "Here we use the wrapper in the models module inside `swirl_dynamics`. This allows to avoid a large part of the boiler plate, that we would otherwise need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMwnFjbzi176"
      },
      "outputs": [],
      "source": [
        "def build_model(config: ConfigDict):\n",
        "  \"\"\"Builds the model from config file.\"\"\"\n",
        "\n",
        "  flow_model = ConvModel()\n",
        "\n",
        "  model = models.ReFlowModel(\n",
        "      # Input shape without the batch dimension.\n",
        "      input_shape=(\n",
        "          config.input_shapes[0][1],\n",
        "          config.input_shapes[0][2],\n",
        "      ),\n",
        "      # Instance of the nn.Module.\n",
        "      flow_model=flow_model,\n",
        "      # Sampling schedule for the reflow time.\n",
        "      time_sampling=functools.partial(\n",
        "        jax.random.uniform, dtype=jax.numpy.float32\n",
        "    )\n",
        "  )\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGGmgMehkO6h"
      },
      "outputs": [],
      "source": [
        "model = build_model(config)\n",
        "init_vars = model.initialize(jax.random.PRNGKey(0))\n",
        "mutables, params = flax.core.pop(init_vars, \"params\")\n",
        "print(\"Checking the keys in the parames. So it was properly initialized\")\n",
        "print(params.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSuF5VeRkJl2"
      },
      "source": [
        "### Building the trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icw8mWoHWjE8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Builds the trainer. Depending if there are more than one GPU/TPU.\n",
        "if config.is_distributed:\n",
        "  trainer_class = trainers.DistributedReFlowTrainer\n",
        "else:\n",
        "  trainer_class = trainers.ReFlowTrainer\n",
        "\n",
        "trainer = trainer_class(\n",
        "        model=model,\n",
        "        rng=jax.random.PRNGKey(config.seed),\n",
        "        optimizer=optimizer,\n",
        "        ema_decay=config.ema_decay,\n",
        "    )\n",
        "\n",
        "# Sets up checkpointing.\n",
        "ckpt_options = checkpoint.CheckpointManagerOptions(\n",
        "    save_interval_steps=config.save_interval_steps,\n",
        "    max_to_keep=config.max_checkpoints_to_keep,\n",
        ")\n",
        "\n",
        "# Sets up the working directory.\n",
        "workdir = \"/content\" # typical current position in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0gC0ChUeh5c"
      },
      "outputs": [],
      "source": [
        "### If you need to remove the checkpoint to start from scratch.\n",
        "# !rm -Rf /content/checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWxXqMZnhE3d"
      },
      "source": [
        "### Running the training loop.\n",
        "\n",
        "We run the training loop.\n",
        "\n",
        "Here the seek to solve the problem\n",
        "\n",
        "$$ \\min_{\\theta} \\mathbb{E}_{t \\sim U[0, 1]}, \\mathbb{E}_{(x_0, x_1) \\in \\mu_0 \\otimes \\mu_1} \\left \\| (x_1 - x_0) - v_{\\theta}(x_t, t)  \\right \\|^2,$$\n",
        "where $x_t = (1-t) x_1 + t x_0$, and $\\mu_0$ and $\\mu_1$ are the distribution for the data stemming from the FV and spectral disctrization respectively.\n",
        "\n",
        "\n",
        "The typical speed of training will greatly depend on the accelerator used.\n",
        "\n",
        "- A100\n",
        "  - with batch size 16: 60 it/s\n",
        "  - with batch size 32: 48 it/s\n",
        "  - with batch size 64: 35 it/s ~ it takes around 13 mins for 25k its."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4MGSJeXaWSe"
      },
      "outputs": [],
      "source": [
        "# Run training loop.\n",
        "train.run(\n",
        "    train_dataloader=train_dataloader,\n",
        "    trainer=trainer,\n",
        "    workdir=workdir,\n",
        "    total_train_steps=config.num_train_steps,\n",
        "    metric_writer=metric_writers.create_default_writer(\n",
        "        workdir, asynchronous=False\n",
        "    ),\n",
        "    metric_aggregation_steps=config.metric_aggregation_steps,  # 30\n",
        "    eval_dataloader=eval_dataloader,\n",
        "    eval_every_steps=config.eval_every_steps,\n",
        "    num_batches_per_eval=config.num_batches_per_eval,\n",
        "    callbacks=(\n",
        "        # This callback saves model checkpoint periodically.\n",
        "        callbacks.TrainStateCheckpoint(\n",
        "            base_dir=workdir,\n",
        "            options=ckpt_options,\n",
        "        ),\n",
        "        # Callback to add the number of iterations/second.\n",
        "        callbacks.TqdmProgressBar(\n",
        "            total_train_steps=config.num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5c3-q7FkZWc"
      },
      "source": [
        "# Running Inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41SUZuGgmCn7"
      },
      "source": [
        "Loading extra libraries for runninf inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oYyp_5Nl_n1"
      },
      "outputs": [],
      "source": [
        "from swirl_dynamics.lib.solvers import ode as ode_solvers\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_poURa9OmVth"
      },
      "outputs": [],
      "source": [
        "num_sampling_steps = 64\n",
        "batch_size_sampling = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc8IMrsclUxQ"
      },
      "source": [
        "Define the dataloader to run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCC3H1bEdMTF"
      },
      "outputs": [],
      "source": [
        "test_dataloader, _ = create_loader_from_ndarray(\n",
        "        dataset=u_lr_lf[-num_test_trajs:].reshape((-1, u_lr_lf.shape[-1], 1)),\n",
        "        batch_size=batch_size_sampling,\n",
        "        shuffle=False,\n",
        "        num_epochs=1,\n",
        "        seed=0,\n",
        "        normalize=config.normalize,\n",
        "        normalize_stats=train_dataloader.normalize_stats_a,\n",
        "        output_name=\"x_0\",\n",
        "        drop_remainder=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QyDzd5LlwIq"
      },
      "source": [
        "### Load the last trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK2Y1RC2lvKQ"
      },
      "outputs": [],
      "source": [
        "# TODO: add the option to download an already trained model.\n",
        "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
        "        f\"{workdir}/checkpoints\", step=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udq4tYKKl4vt"
      },
      "outputs": [],
      "source": [
        "latent_dynamics_fn = ode_solvers.nn_module_to_dynamics(\n",
        "      model.flow_model, autonomous=False, is_training=False\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgweIuX4l9GF"
      },
      "outputs": [],
      "source": [
        "integrator = ode_solvers.RungeKutta4()\n",
        "integrate_fn = jax.jit(functools.partial(\n",
        "    integrator,\n",
        "    latent_dynamics_fn,\n",
        "    tspan=jnp.arange(0.0, 1.0, 1.0 / num_sampling_steps),\n",
        "    params=trained_state.model_variables,\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hf0XVLJqH_F"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(test_dataloader))\n",
        "print(batch['x_0'].shape)\n",
        "out_put = integrate_fn(batch['x_0'])\n",
        "print(out_put.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJzKJdGFnESD"
      },
      "source": [
        "### Running Inference Loop.\n",
        "\n",
        "This steps solves the ODE\n",
        "$$\\dot{x} = v_{\\theta}(x, t),$$\n",
        "with initial condition $x(0) = x_0$. For all the $x_0 \\sim \\mu_0$ in the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2vqQkvUmrk7"
      },
      "outputs": [],
      "source": [
        "input_list = []\n",
        "output_list = []\n",
        "\n",
        "mean_output = train_dataloader.normalize_stats_b[\"mean\"]\n",
        "std_output = train_dataloader.normalize_stats_b[\"std\"]\n",
        "\n",
        "for ii, batch in tqdm(enumerate(test_dataloader)):\n",
        "  input_list.append(batch[\"x_0\"])\n",
        "\n",
        "  output = np.array(\n",
        "          integrate_fn(batch[\"x_0\"])[-1].reshape(\n",
        "              (-1, config.input_shapes[1][1], config.input_shapes[1][2])\n",
        "          )\n",
        "      )\n",
        "  # Denormalizes the output. If the model was trained with normalized data.\n",
        "  if config.normalize:\n",
        "    output = std_output * output + mean_output\n",
        "  output_list.append(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tcgQ7Yq1kjE"
      },
      "source": [
        "### Evaluation of the transformed data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uzp_fHPoq4Lk"
      },
      "outputs": [],
      "source": [
        "# Defining the ground truth, the input and the output to compute the point-wise\n",
        "# statistics, in a particular point of the domain.\n",
        "output_array = np.concatenate(\n",
        "    output_list, axis=0).reshape(16, -1, config.input_shapes[1][1])\n",
        "output = output_array.reshape(-1, config.input_shapes[1][1])\n",
        "test_u_lr_lf = u_lr_lf[-num_test_trajs:].reshape(-1, 48)\n",
        "test_u_lr_hf = u_lr_hf[-num_test_trajs:].reshape(-1, 48)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6C3ldB75yTk"
      },
      "source": [
        "We plot a simple histogram to check the difference in the marginal statistics of on particular point. As the problem is translation equivariant this is enough.\n",
        "\n",
        "This is similar to the results obtained using a Sinkhorn iteration solver when solving an Optimal Transport problem for debiasing see this [notebook](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/debiasing/optimal_transport/colab/optimal_transport_sinkhorn.ipynb). The main advantage of this approach is that we don't need to store the fullt training data, which also allows to process much larger amount of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHL2corlse1t"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "idx_x = 2\n",
        "plt.hist(test_u_lr_lf[:, idx_x], bins=50, density=True,\n",
        "         alpha=0.5, label='Finite Volumes')\n",
        "plt.hist(test_u_lr_hf[:, idx_x], bins=50, density=True,\n",
        "         alpha=0.5, label='Spectral')\n",
        "plt.hist(output[:, idx_x], bins=50, density=True,\n",
        "         alpha=0.5, label='Reflow')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkGFwlFbDAqT"
      },
      "source": [
        "We define functions to compare the energy spectra of the different datasets, and the debiased data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QK2FeAaAVF0"
      },
      "outputs": [],
      "source": [
        "def _energy(x: jax.Array) -\u003e jax.Array:\n",
        "  \"\"\"Computes energy of a given array.\"\"\"\n",
        "  return jnp.square(jnp.abs(jnp.fft.fftshift(jnp.fft.fft(x))))\n",
        "\n",
        "def energy_spectra(\n",
        "    num_bins: int = 24, nx: int = 48\n",
        ") -\u003e tuple[Callable[[jax.Array], jax.Array], jax.Array]:\n",
        "  \"\"\"Computes radial spectra of a given array.\n",
        "\n",
        "  Args:\n",
        "    num_bins: Number of bins for the energy spectrum.\n",
        "    lon: Number of discretization point in the longitudinal direction.\n",
        "    lat: Number of discretization points in the meridional direction.\n",
        "\n",
        "  Returns:\n",
        "    A tuple with the function that computes the radial spectra and the\n",
        "    frequencies associated to each bin (without the 0-th frequency)\n",
        "  \"\"\"\n",
        "  kx = np.fft.fftshift(np.fft.fftfreq(nx, d=1 / nx))\n",
        "  k = np.abs(kx)\n",
        "  bins = np.linspace(np.min(k), np.max(k), num=num_bins)\n",
        "  indices = np.digitize(k, bins)\n",
        "\n",
        "  def _radial_spectra(x):\n",
        "    energy = _energy(x)\n",
        "    energy_k = lambda kk: jnp.sum((indices == kk) * energy)\n",
        "    rad_energy = jax.vmap(energy_k)(jnp.arange(1, num_bins))\n",
        "    return rad_energy\n",
        "\n",
        "  return jax.jit(_radial_spectra), bins[1:]\n",
        "\n",
        "def build_vectorized_function(\n",
        "    num_bins: int = 24, nx: int = 48\n",
        ") -\u003e Callable[[jax.Array], jax.Array]:\n",
        "  \"\"\"Builds vectorized function radial spectra.\n",
        "\n",
        "  Args:\n",
        "    num_bins: Number of bins for the energy spectrum.\n",
        "    lon: Number of discretization point in the longitudinal direction.\n",
        "    lat: Number of discretization points in the meridional direction.\n",
        "\n",
        "  Returns:\n",
        "    A function that computes the radial spectra of a given array.\n",
        "  \"\"\"\n",
        "  rad_spec_fn, _ = energy_spectra(num_bins, nx)\n",
        "  vmapped_radspec = jax.vmap(rad_spec_fn)\n",
        "\n",
        "  def sample_radspec(samples: jax.Array) -\u003e jax.Array:\n",
        "    spec = vmapped_radspec(samples)\n",
        "    return np.mean(spec, axis=0)\n",
        "\n",
        "  return sample_radspec\n",
        "\n",
        "def mean_log_ratio(\n",
        "    radspec1: jax.Array, radspec2: jax.Array, range_freq: slice\n",
        ") -\u003e jax.Array:\n",
        "  \"\"\"Computes the mean log ratio of two radial spectra.\n",
        "\n",
        "  Args:\n",
        "    radspec1: First radial spectra.\n",
        "    radspec2: Second radial spectra.\n",
        "    range_freq: Range of frequencies to compute the error.\n",
        "\n",
        "  Returns:\n",
        "    The mean log ratio between the two radial spectra.\n",
        "  \"\"\"\n",
        "  return jnp.mean(\n",
        "      jnp.abs(jnp.log10(radspec1[range_freq] / radspec2[range_freq]))\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxo5x0EiCqEr"
      },
      "source": [
        "Comparing the log ratio of the energy spectra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l53_M6l_B8yx"
      },
      "outputs": [],
      "source": [
        "fun_vect = build_vectorized_function()\n",
        "# Computes the mean energy spectra.\n",
        "spectrum_lf = fun_vect(test_u_lr_lf)\n",
        "spectrum_hf = fun_vect(test_u_lr_hf)\n",
        "spectrum_reflow = fun_vect(output)\n",
        "\n",
        "# Computes the mean log ratio.\n",
        "err_log_ratio_ref = mean_log_ratio(spectrum_lf,\n",
        "                                   spectrum_hf,\n",
        "                                   range_freq = None)\n",
        "err_log_ratio_reflow = mean_log_ratio(spectrum_reflow,\n",
        "                                      spectrum_hf,\n",
        "                                      range_freq = None)\n",
        "\n",
        "print(\"Error of the log ratio of the energy spectrum of FV data\",\n",
        "      f\"{err_log_ratio_ref}\")\n",
        "print(\"Error of the log ratio of the energy spectrum of debiased FV data\",\n",
        "      f\"{err_log_ratio_reflow}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnChBDzVAaFa"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1LaxpTzo8aucfAAj5eI8buNWY4pDI_T6d",
          "timestamp": 1737956826407
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
