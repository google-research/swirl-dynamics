{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htEEY6RvQ8Tp"
      },
      "source": [
        "# Example for generation using stochastic interpolants.\n",
        "\n",
        "Here we consider the simple example of generating numbers (from MNIST) using the stochasting interpolant formalism.\n",
        "\n",
        "We consider the simplest instantiation of the stochastic interpolants, which coincides with rectified flows.\n",
        "\n",
        "Basically, suppose that we have two distributions of $d$-dimensional vectors $X_0$ and $X_1$, then we define the interpolant:\n",
        "$$X_t = (1 -t) X_0 + t X_1$$\n",
        "for $t \\in [0, 1]$. Here we consider $X_0 \\sim N(0, I_{d})$ and $X_1$ is random variable given by handwritten digits, with samples taken from the MNIST dataset. Here $d$ is the number of pixels of MNIST samples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcRQQ_bZTzaf"
      },
      "source": [
        "### Downloading dependencies.\n",
        "\n",
        "We use the `swirl-dynamics` library for most of the heavy lifting, so we install it using pip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFKh8QMPRKuv"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/google-research/swirl-dynamics.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suLdZxuhE4qv"
      },
      "source": [
        "We also import all the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjF7tVIERgGH"
      },
      "outputs": [],
      "source": [
        "from clu import metric_writers\n",
        "import jax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "from orbax import checkpoint\n",
        "from swirl_dynamics.projects.debiasing.rectified_flow import models as reflow_models\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import interpolants\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import losses\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import models\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import trainers\n",
        "from swirl_dynamics.templates import callbacks\n",
        "from swirl_dynamics.templates import train\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn6RrtNTU-ew"
      },
      "source": [
        "### Define Hyper-Parameters\n",
        "\n",
        "For simplicity we define the parameters inside a `ConfigDict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLz7hEEGUrE2"
      },
      "outputs": [],
      "source": [
        "import ml_collections\n",
        "\n",
        "config = ml_collections.ConfigDict()\n",
        "\n",
        "# Parameters for the training steps.\n",
        "config.initial_lr = 1e-6\n",
        "config.peak_lr = 1e-4\n",
        "config.warmup_steps = 10_000\n",
        "config.num_train_steps = 100_000\n",
        "config.end_lr = 1e-6\n",
        "config.beta1 = 0.999\n",
        "config.clip = 1.0\n",
        "config.save_interval_steps = 1000\n",
        "config.max_checkpoints_to_keep = 10\n",
        "\n",
        "config.num_train_steps = 50_000\n",
        "config.metric_aggregation_steps = 1000\n",
        "config.eval_every_steps = 10_000\n",
        "config.num_batches_per_eval = 2\n",
        "config.batch_size_training = 64\n",
        "config.batch_size_eval = 32\n",
        "\n",
        "# Parameters for the instantation of the neural network.\n",
        "# Here we will use a simple convoluational U-net with FilM layers\n",
        "config.out_channels = 1\n",
        "config.num_channels = (64, 128)\n",
        "config.downsample_ratio = (2, 2)\n",
        "config.num_blocks = 4\n",
        "config.noise_embed_dim = 128\n",
        "config.padding = \"SAME\"\n",
        "config.use_attention = True\n",
        "config.use_position_encoding = True\n",
        "config.num_heads = 8\n",
        "config.sigma_data = 0.31\n",
        "config.seed = 666\n",
        "config.ema_decay = 0.99\n",
        "\n",
        "# The shapes of x_0 and x_1.\n",
        "# The leading one represents the batch dimension.\n",
        "config.input_shapes = ((1, 28, 28, 1), (1, 28, 28, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrXypPLGUGXg"
      },
      "source": [
        "### Downloading the data.\n",
        "\n",
        "For the data we leverage the MNIST dataset in tensorflow datasets, to which we introduce an extra field with random Normal noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fa5rMtlFRdGI"
      },
      "outputs": [],
      "source": [
        "def get_mnist_dataset(split: str, batch_size: int, repeat: bool = True):\n",
        "  ds = tfds.load(\"mnist\", split=split)\n",
        "  ds = ds.map(\n",
        "      # Change field name from \"image\" to \"x\" (required by `DenoisingModel`)\n",
        "      # and normalize the value to [0, 1].\n",
        "      lambda x: {\n",
        "          \"x_0\": tf.random.normal(shape=x[\"image\"].shape, mean=0.0),\n",
        "          \"x_1\": tf.cast(x[\"image\"], tf.float32) / 255.0,\n",
        "      }\n",
        "  )\n",
        "  if repeat:\n",
        "    ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  ds = ds.as_numpy_iterator()\n",
        "  return ds\n",
        "\n",
        "\n",
        "# The standard deviation of the normalized dataset.\n",
        "# This is useful for determining the diffusion scheme and preconditioning\n",
        "# of the neural network parametrization.\n",
        "DATA_STD = 0.31"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quW2jj96Odma"
      },
      "source": [
        "Instantiating the dataloaders. This will download the data to disk so it can be fed directly to the training pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMwnFjbzi176"
      },
      "outputs": [],
      "source": [
        "train_dataloader = get_mnist_dataset(\n",
        "    split=\"train\", batch_size=config.batch_size_training\n",
        ")\n",
        "eval_dataloader = get_mnist_dataset(\n",
        "    split=\"test\", batch_size=config.batch_size_eval\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg9o9wAxO9BO"
      },
      "source": [
        "Here we extract one batch and we probe the elements inside a batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Se3Ur0TO9e6"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(f\"Keys of the batch: {batch.keys()}\")\n",
        "print(f\"Shape of the x_0: {batch['x_0'].shape}\")\n",
        "print(f\"Shape of the x_1: {batch['x_1'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJdCaY59PTQb"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(batch[\"x_0\"][1, :, :, 0])\n",
        "plt.title(\"Sample from initial distribution x_0\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(batch[\"x_1\"][1, :, :, 0])\n",
        "plt.title(\"Sample from target distribution x_1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlyDc4ykUY0S"
      },
      "source": [
        "### Defining the stochastic interpolant optimizers\n",
        "\n",
        "Here we define the learning rate schedule, for simplicity we use a linear ramp-up followed with a cosine decay schedule. This can be further tweaked but empirically, this has shown to provide reasonable results for this type of problems.\n",
        "\n",
        "For the optimizer we use the Adam optimizer, and we also add a clipping mechanism to help avoid instabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr9nX8QvUfrl"
      },
      "outputs": [],
      "source": [
        "# Defining experiments through the config file.\n",
        "schedule = optax.warmup_cosine_decay_schedule(\n",
        "    init_value=config.initial_lr,\n",
        "    peak_value=config.peak_lr,\n",
        "    warmup_steps=config.warmup_steps,\n",
        "    decay_steps=config.num_train_steps,\n",
        "    end_value=config.end_lr,\n",
        ")\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.adam(\n",
        "        learning_rate=schedule,\n",
        "        b1=config.beta1,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3HHc7egnxWl"
      },
      "source": [
        "## Instantiating the model\n",
        "\n",
        "In this case the model is a fully convolutional U-net model, using ResNet blocks with a Fourier embedding layer for the time.\n",
        "\n",
        "Here this model parametrized the velocity vector field in the stochastic interpolant framework.\n",
        "\n",
        "I.e., we have an interpolant of the form:\n",
        "$$x_t = \\alpha(t) x_0 + \\beta(y) x_1 $$\n",
        "where\n",
        "$$\\alpha(t) = 1-t, \\qquad \\text{and} \\qquad \\beta(t) = t.$$\n",
        "\n",
        "Here we use the already defined ``LinerInterpolant`` class defined in [``interpolants``](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/debiasing/stochastic_interpolants/interpolants.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFvFZeNxjPaj"
      },
      "outputs": [],
      "source": [
        "interpolant = interpolants.LinearInterpolant()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6y-uxDimu7K"
      },
      "source": [
        "We show how the interpolant progressible transforms the Gaussian noise to one of the target samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcJWoT5vkorl"
      },
      "outputs": [],
      "source": [
        "x_0_dummy = batch['x_0'][0:1, ..., 0]\n",
        "x_1_dummy = batch['x_1'][0:1, ..., 0]\n",
        "\n",
        "t_array = jnp.linspace(0, 1, 6)\n",
        "fig, axs = plt.subplots(1, 6, figsize=(24, 4))\n",
        "for ii, t in enumerate(t_array):\n",
        "  x_t = interpolant(t[None], x_0_dummy, x_1_dummy)\n",
        "  axs[ii].imshow(x_t[0, :, :])\n",
        "  axs[ii].set_title(f'Sample from x_{t:\u003c.3f}')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_BSg52hjTCl"
      },
      "source": [
        "We consider a generative model that is instantiated by solving the following ODE:\n",
        "$$\\dot{x} = v_{\\theta}(x, t), \\qquad t \\in [0, 1],$$\n",
        "in this case, the model defined below parametrizes $v_{\\theta}(x, t)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocXSM5oEnwj7"
      },
      "outputs": [],
      "source": [
        "flow_model = reflow_models.RescaledUnet(\n",
        "    out_channels=1,\n",
        "    num_channels=(64, 128),\n",
        "    downsample_ratio=(2, 2),\n",
        "    num_blocks=4,\n",
        "    noise_embed_dim=128,\n",
        "    padding=\"SAME\",\n",
        "    use_attention=True,\n",
        "    use_position_encoding=True,\n",
        "    num_heads=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3AYca0-nqVv"
      },
      "source": [
        "We also need to measure how the distance between the neural network and the speed would be considered. In this case we consider the loss:\n",
        "$$|v_{\\theta}(x_t, t) - \\dot{x}_{t}|^2,$$\n",
        "which can be furter simplified to\n",
        "$$|v_{\\theta}(x_t, t) - (x_1 - x_0)|^2.$$\n",
        "using the fact that $\\dot{x}_{t} = x_1 - x_0.$\n",
        "\n",
        "An equivalent loss was already defined in the [``losses``](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/debiasing/stochastic_interpolants/losses.py) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcd9wXZBnpy5"
      },
      "outputs": [],
      "source": [
        "loss_stochastic_interpolant = losses.velocity_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u9Xc-URnaYi"
      },
      "source": [
        "Now we have all the required elements to create an instance of ``StochasticInterpolantModel``, which encapsulates all the information at the model level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md1p7pzNnVwW"
      },
      "outputs": [],
      "source": [
        "model = models.StochasticInterpolantModel(\n",
        "    input_shape=(\n",
        "        config.input_shapes[0][1],\n",
        "        config.input_shapes[0][2],\n",
        "        config.input_shapes[0][3],\n",
        "    ),  # This must agree with the expected sample shape.\n",
        "    flow_model=flow_model,\n",
        "    # Defines the type of stochastic interpolant.\n",
        "    interpolant=interpolant,\n",
        "    # Defines the type of loss used for the training.\n",
        "    loss_stochastic_interpolant=loss_stochastic_interpolant,\n",
        "    num_eval_cases_per_lvl=8,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSuF5VeRkJl2"
      },
      "source": [
        "### Building the trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaSwHJJuo1f1"
      },
      "source": [
        "Now, we just need to instantiate the trainer, which contains all the information to run the training loop. This includes the model, the optimizer, and the checkpointer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icw8mWoHWjE8"
      },
      "outputs": [],
      "source": [
        "# Defining the trainer.\n",
        "trainer = trainers.StochasticInterpolantTrainer(\n",
        "    model=model,\n",
        "    rng=jax.random.key(config.seed),\n",
        "    optimizer=optimizer,\n",
        "    ema_decay=config.ema_decay,\n",
        ")\n",
        "\n",
        "# Setting up checkpointing.\n",
        "ckpt_options = checkpoint.CheckpointManagerOptions(\n",
        "    save_interval_steps=config.save_interval_steps,\n",
        "    max_to_keep=config.max_checkpoints_to_keep,\n",
        ")\n",
        "\n",
        "# Sets up the working directory.\n",
        "workdir = \"/content\"  # typical current position in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0gC0ChUeh5c"
      },
      "outputs": [],
      "source": [
        "### If you need to remove the checkpoint to start from scratch.\n",
        "# !rm -Rf /content/checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWxXqMZnhE3d"
      },
      "source": [
        "### Running the training loop.\n",
        "\n",
        "We run the training loop.\n",
        "\n",
        "Here the seek to solve the problem\n",
        "\n",
        "$$ \\min_{\\theta} \\mathbb{E}_{t \\sim U[0, 1]} \\mathbb{E}_{(x_0, x_1) \\in \\mu_0 \\otimes \\mu_1} \\left \\| \\dot{x}_t - v_{\\theta}(x_t, t)  \\right \\|^2,$$\n",
        "where $x_t = t x_1 + (1-t) x_0$, and $\\mu_0 \\sim N(0, 1)$ and $\\mu_1$ is the distribution of MNIST digits.\n",
        "\n",
        "\n",
        "This loss can be further simplified as\n",
        "$$ \\min_{\\theta} \\mathbb{E}_{t \\sim U[0, 1]} \\mathbb{E}_{(x_0, x_1) \\in \\mu_0 \\otimes \\mu_1}   | v_{\\theta}(x_t, t)|^2  - 2 (x_1 - x_0) \\cdot  v_{\\theta}(x_t, t),$$\n",
        "using the fact that $\\dot{x}_t = x_1 - x_0$ and that $\\dot{x}_t$ is independent of $\\theta$.\n",
        "\n",
        "Note the full training step it takes around 15-20 mins in a TPU v6e (Trillium).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4MGSJeXaWSe"
      },
      "outputs": [],
      "source": [
        "# Run training loop.\n",
        "\n",
        "train.run(\n",
        "    train_dataloader=train_dataloader,\n",
        "    trainer=trainer,\n",
        "    workdir=workdir,\n",
        "    total_train_steps=config.num_train_steps,\n",
        "    metric_aggregation_steps=config.metric_aggregation_steps,  # 30\n",
        "    eval_dataloader=eval_dataloader,\n",
        "    eval_every_steps=config.eval_every_steps,\n",
        "    num_batches_per_eval=config.num_batches_per_eval,\n",
        "    metric_writer=metric_writers.create_default_writer(\n",
        "        workdir, asynchronous=False\n",
        "    ),\n",
        "    callbacks=(\n",
        "        callbacks.TqdmProgressBar(\n",
        "            total_train_steps=config.num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        # This callback saves model checkpoint periodically.\n",
        "        callbacks.TrainStateCheckpoint(\n",
        "            base_dir=workdir,\n",
        "            options=ckpt_options,\n",
        "        ),\n",
        "        # TODO add a plot callback.\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5c3-q7FkZWc"
      },
      "source": [
        "# Running Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41SUZuGgmCn7"
      },
      "source": [
        "Loading extra libraries for running inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oYyp_5Nl_n1"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "from swirl_dynamics.lib.solvers import ode as ode_solvers\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc8IMrsclUxQ"
      },
      "source": [
        "Define the dataloader to run inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCC3H1bEdMTF"
      },
      "outputs": [],
      "source": [
        "test_dataloader = get_mnist_dataset(\n",
        "    split=\"test\", batch_size=config.batch_size_eval, repeat=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QyDzd5LlwIq"
      },
      "source": [
        "### Load the last trained model and define the dynamics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WK2Y1RC2lvKQ"
      },
      "outputs": [],
      "source": [
        "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
        "    f\"{workdir}/checkpoints\", step=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Udq4tYKKl4vt"
      },
      "outputs": [],
      "source": [
        "latent_dynamics_fn = ode_solvers.nn_module_to_dynamics(\n",
        "    model.flow_model,\n",
        "    autonomous=False,\n",
        "    is_training=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU8-AFJep_5X"
      },
      "source": [
        "We define the ODE solver, (here Runge-Kutta 4th order), and other details such as the number of steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgweIuX4l9GF"
      },
      "outputs": [],
      "source": [
        "num_sampling_steps = 128\n",
        "\n",
        "integrator = ode_solvers.RungeKutta4()\n",
        "integrate_fn = functools.partial(\n",
        "    integrator,\n",
        "    latent_dynamics_fn,\n",
        "    tspan=jnp.arange(0.0, 1.0, 1.0 / num_sampling_steps),\n",
        "    params=trained_state.model_variables,\n",
        ")\n",
        "\n",
        "integrate_fn_jit = jax.jit(integrate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hf0XVLJqH_F"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(test_dataloader))\n",
        "print(f\"Shape of the x_0 condition, {batch['x_0'].shape}\")\n",
        "out_put = integrate_fn_jit(batch[\"x_0\"])\n",
        "print(f\"Shape of the generated x_1 {out_put.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a19KqEGISluG"
      },
      "outputs": [],
      "source": [
        "plt.imshow(out_put[-1, 0, :, :, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJzKJdGFnESD"
      },
      "source": [
        "### Running Inference Loop.\n",
        "\n",
        "This may take a non-negligible amount of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2vqQkvUmrk7"
      },
      "outputs": [],
      "source": [
        "input_list = []\n",
        "output_list = []\n",
        "\n",
        "for ii, batch in tqdm(enumerate(test_dataloader)):\n",
        "  input_list.append(batch[\"x_0\"])\n",
        "\n",
        "  output = np.array(\n",
        "      integrate_fn_jit(batch[\"x_0\"])[-1].reshape(\n",
        "          (-1, config.input_shapes[1][1], config.input_shapes[1][2])\n",
        "      )\n",
        "  )\n",
        "  output_list.append(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4bBlRPeYz-P"
      },
      "outputs": [],
      "source": [
        "output_array = np.concatenate(output_list, axis=0)\n",
        "print(f\"Shape of the output array: {output_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INuCK8YBY7iq"
      },
      "outputs": [],
      "source": [
        "num_plots = 6\n",
        "fig, axs = plt.subplots(1, num_plots, figsize=(num_plots * 4, 4))\n",
        "num_samples = output_array.shape[0]\n",
        "idx_samples = np.linspace(0, num_samples - 1, num_plots).astype(int)\n",
        "for ii, idx in enumerate(idx_samples):\n",
        "  axs[ii].imshow(output_array[idx, :, :])\n",
        "  axs[ii].set_title(f\"Sample number: {idx}\")\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1EMyBTRxEvF-XHnKQK3HNgWvqKYNpzAfi",
          "timestamp": 1745815061659
        },
        {
          "file_id": "1LaxpTzo8aucfAAj5eI8buNWY4pDI_T6d",
          "timestamp": 1745555288947
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
