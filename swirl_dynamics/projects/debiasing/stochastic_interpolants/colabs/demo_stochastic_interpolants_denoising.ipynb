{
  "cells": [
    {
      "metadata": {
        "id": "htEEY6RvQ8Tp"
      },
      "cell_type": "markdown",
      "source": [
        "# Denoising example using stochastic interpolants.\n",
        "\n",
        "Here we consider the simple example of unblurring handwriten digits from the MNIST dataset using the stochasting interpolant formalism.\n",
        "\n",
        "We consider the simplest instantiation of the stochastic interpolants, which coincides with rectified flows to which we add the so called 'latent' field $Z \\sim N(0, \\sigma^2)$.\n",
        "\n",
        "We have two distributions of $d$-dimensional vectors $X_0$ and $X_1$, then we define the interpolant:\n",
        "$$X_t = (1 -t) X_0 + t X_1 + \\gamma(t) Z $$\n",
        "for $t \\in [0, 1]$. Here we consider $X_0$ to be blurred handwritten digits with a Gaussian kernel, and $X_1$ are the handwritten digits, with samples taken from the MNIST dataset. Here $d$ is the number of pixels of MNIST samples.\n"
      ]
    },
    {
      "metadata": {
        "id": "LcRQQ_bZTzaf"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading dependencies.\n",
        "\n",
        "We use the `swirl-dynamics` library for most of the heavy lifting, so we install it using pip."
      ]
    },
    {
      "metadata": {
        "id": "nFKh8QMPRKuv"
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/google-research/swirl-dynamics.git@main"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "suLdZxuhE4qv"
      },
      "cell_type": "markdown",
      "source": [
        "We also import all the necessary libraries."
      ]
    },
    {
      "metadata": {
        "id": "zjF7tVIERgGH"
      },
      "cell_type": "code",
      "source": [
        "from clu import metric_writers\n",
        "import jax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "from orbax import checkpoint\n",
        "from swirl_dynamics.projects.debiasing.rectified_flow import models as reflow_models\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import interpolants\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import losses\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import models\n",
        "from swirl_dynamics.projects.debiasing.stochastic_interpolants import trainers\n",
        "from swirl_dynamics.templates import callbacks\n",
        "from swirl_dynamics.templates import train\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Tn6RrtNTU-ew"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Hyper-Parameters\n",
        "\n",
        "For simplicity we define the parameters inside a `ConfigDict`."
      ]
    },
    {
      "metadata": {
        "id": "jLz7hEEGUrE2"
      },
      "cell_type": "code",
      "source": [
        "import ml_collections\n",
        "\n",
        "config = ml_collections.ConfigDict()\n",
        "\n",
        "# Parameters for the training steps.\n",
        "config.initial_lr = 1e-6\n",
        "config.peak_lr = 1e-4\n",
        "config.warmup_steps = 10_000\n",
        "config.num_train_steps = 100_000\n",
        "config.end_lr = 1e-6\n",
        "config.beta1 = 0.999\n",
        "config.clip = 1.0\n",
        "config.save_interval_steps = 1000\n",
        "config.max_checkpoints_to_keep = 10\n",
        "\n",
        "config.num_train_steps = 50_000\n",
        "config.metric_aggregation_steps = 1000\n",
        "config.eval_every_steps = 10_000\n",
        "config.num_batches_per_eval = 2\n",
        "config.batch_size_training = 64\n",
        "config.batch_size_eval = 32\n",
        "\n",
        "# Parameters for the instantation of the neural network.\n",
        "# Here we will use a simple convoluational U-net with FilM layers\n",
        "config.out_channels = 1\n",
        "config.num_channels = (64, 128)\n",
        "config.downsample_ratio = (2, 2)\n",
        "config.num_blocks = 4\n",
        "config.noise_embed_dim = 128\n",
        "config.padding = \"SAME\"\n",
        "config.use_attention = True\n",
        "config.use_position_encoding = True\n",
        "config.num_heads = 8\n",
        "config.sigma_data = 0.31\n",
        "config.seed = 666\n",
        "config.ema_decay = 0.99\n",
        "\n",
        "# The shapes of x_0 and x_1.\n",
        "# The leading one represents the batch dimension.\n",
        "config.input_shapes = ((1, 28, 28, 1), (1, 28, 28, 1))"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5_fvwh-S8PhB"
      },
      "cell_type": "markdown",
      "source": [
        "Creating a simple blurring kernel"
      ]
    },
    {
      "metadata": {
        "id": "9thofbci8O5V"
      },
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def gaussian_blur(\n",
        "    image: tf.Tensor, kernel_size: int = 7, sigma: float = 1.0\n",
        ") -\u003e tf.Tensor:\n",
        "  \"\"\"Performs a Gaussian blur on an image tensor without using tf.addons.\n",
        "\n",
        "  Args:\n",
        "    image: A 4D tensor of shape [batch_size, height, width, channels].\n",
        "    kernel_size: The size of the Gaussian kernel. Must be an odd integer.\n",
        "    sigma: The standard deviation of the Gaussian kernel.\n",
        "\n",
        "  Returns:\n",
        "    A 4D tensor of the same shape as the input image, with the Gaussian blur\n",
        "    applied.\n",
        "  \"\"\"\n",
        "  if kernel_size % 2 == 0:\n",
        "    raise ValueError(\n",
        "        \"The value of kernel_size must be an odd integer\",\n",
        "        f\"instead we have {kernel_size}\",\n",
        "    )\n",
        "\n",
        "  # This is roundabout to use depthwise_conv2d on a 3-tensor.\n",
        "  image = tf.expand_dims(image, axis=0)\n",
        "\n",
        "  # Create a 1D Gaussian kernel.\n",
        "  x = tf.range(-kernel_size // 2 + 1, kernel_size // 2 + 1, dtype=tf.float32)\n",
        "  g = tf.exp(-(tf.pow(x, 2) / (2 * tf.pow(tf.constant(sigma), 2))))\n",
        "  g_norm = g / tf.reduce_sum(g)\n",
        "\n",
        "  # Create a 2D Gaussian kernel from the 1D kernel.\n",
        "  kernel = tf.einsum(\"i,j-\u003eij\", g_norm, g_norm)\n",
        "  kernel = tf.expand_dims(tf.expand_dims(kernel, axis=-1), axis=-1)\n",
        "\n",
        "  # Get the number of channels in the input image.\n",
        "  num_channels = tf.shape(image)[-1]\n",
        "\n",
        "  # Tile the kernel to have the same number of input and output channels.\n",
        "  kernel = tf.tile(kernel, [1, 1, num_channels, 1])\n",
        "\n",
        "  # Apply the convolution.\n",
        "  blurred_image = tf.nn.depthwise_conv2d(\n",
        "      image, kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n",
        "  )\n",
        "\n",
        "  return blurred_image[0]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "xrXypPLGUGXg"
      },
      "cell_type": "markdown",
      "source": [
        "### Downloading the data.\n",
        "\n",
        "For the data we leverage the MNIST dataset in tensorflow datasets, to which we introduce an extra field with random Normal noise."
      ]
    },
    {
      "metadata": {
        "id": "fa5rMtlFRdGI"
      },
      "cell_type": "code",
      "source": [
        "def get_mnist_dataset(split: str, batch_size: int, repeat: bool = True):\n",
        "  ds = tfds.load(\"mnist\", split=split)\n",
        "  ds = ds.map(\n",
        "      # Change field name from \"image\" to \"x\" (required by `DenoisingModel`)\n",
        "      # normalize the value to [0, 1], and add the Gaussian filter.\n",
        "      lambda x: {\n",
        "          \"x_0\": gaussian_blur(\n",
        "              tf.cast(x[\"image\"], tf.float32) / 255.0, sigma=2.0\n",
        "          ),\n",
        "          \"x_1\": tf.cast(x[\"image\"], tf.float32) / 255.0,\n",
        "      }\n",
        "  )\n",
        "  if repeat:\n",
        "    ds = ds.repeat()\n",
        "  ds = ds.batch(batch_size)\n",
        "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "  ds = ds.as_numpy_iterator()\n",
        "  return ds\n",
        "\n",
        "\n",
        "# The standard deviation of the normalized dataset.\n",
        "# This is useful for determining the diffusion scheme and preconditioning\n",
        "# of the neural network parametrization.\n",
        "DATA_STD = 0.31"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "quW2jj96Odma"
      },
      "cell_type": "markdown",
      "source": [
        "Instantiating the dataloaders. This will download the data to disk so it can be fed directly to the training pipeline."
      ]
    },
    {
      "metadata": {
        "id": "NMwnFjbzi176"
      },
      "cell_type": "code",
      "source": [
        "train_dataloader = get_mnist_dataset(\n",
        "    split=\"train\", batch_size=config.batch_size_training\n",
        ")\n",
        "eval_dataloader = get_mnist_dataset(\n",
        "    split=\"test\", batch_size=config.batch_size_eval\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Dg9o9wAxO9BO"
      },
      "cell_type": "markdown",
      "source": [
        "Here we extract one batch and we probe the elements inside a batch.\n"
      ]
    },
    {
      "metadata": {
        "id": "8Se3Ur0TO9e6"
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "print(f\"Keys of the batch: {batch.keys()}\")\n",
        "print(f\"Shape of the x_0: {batch['x_0'].shape}\")\n",
        "print(f\"Shape of the x_1: {batch['x_1'].shape}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "tJdCaY59PTQb"
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(batch[\"x_0\"][11, :, :, 0])\n",
        "plt.title(\"Sample from initial distribution x_0\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(batch[\"x_1\"][11, :, :, 0])\n",
        "plt.title(\"Sample from target distribution x_1\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "UlyDc4ykUY0S"
      },
      "cell_type": "markdown",
      "source": [
        "### Defining the stochastic interpolant optimizers\n",
        "\n",
        "Here we define the learning rate schedule, for simplicity we use a linear ramp-up followed with a cosine decay schedule. This can be further tweaked but empirically, this has shown to provide reasonable results for this type of problems.\n",
        "\n",
        "For the optimizer we use the Adam optimizer, and we also add a clipping mechanism to help avoid instabilities.\n"
      ]
    },
    {
      "metadata": {
        "id": "Yr9nX8QvUfrl"
      },
      "cell_type": "code",
      "source": [
        "# Defining experiments through the config file.\n",
        "schedule = optax.warmup_cosine_decay_schedule(\n",
        "    init_value=config.initial_lr,\n",
        "    peak_value=config.peak_lr,\n",
        "    warmup_steps=config.warmup_steps,\n",
        "    decay_steps=config.num_train_steps,\n",
        "    end_value=config.end_lr,\n",
        ")\n",
        "\n",
        "optimizer = optax.chain(\n",
        "    optax.adam(\n",
        "        learning_rate=schedule,\n",
        "        b1=config.beta1,\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "p3HHc7egnxWl"
      },
      "cell_type": "markdown",
      "source": [
        "## Instantiating the model\n",
        "\n",
        "In this case the model is a fully convolutional U-net model, using ResNet blocks with a Fourier embedding layer for the time.\n",
        "\n",
        "Here this model parametrized the velocity vector field in the stochastic interpolant framework.\n",
        "\n",
        "I.e., we have an interpolant of the form:\n",
        "$$x_t = \\alpha(t) x_0 + \\beta(t) x_1 + \\gamma(t) z$$\n",
        "where\n",
        "$$\\alpha(t) = 1-t, \\qquad \\beta(t) = t, \\qquad \\text{and} \\qquad \\gamma(t) = \\sin^2 (\\pi t)$$\n",
        "and the latent noise is given by $z \\sim N(0, \\sigma)$ where sigma is often hand tuned in order to render the problem smoother without eliminating all the information from one distribution.\n",
        "\n",
        "Here we use the already defined ``LinerInterpolant`` class defined in [``interpolants``](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/debiasing/stochastic_interpolants/interpolants.py)."
      ]
    },
    {
      "metadata": {
        "id": "jFvFZeNxjPaj"
      },
      "cell_type": "code",
      "source": [
        "sigma = 0.1  # @param\n",
        "interpolant = interpolants.LinearInterpolantSinusoidalNoise(sigma=sigma)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "C6y-uxDimu7K"
      },
      "cell_type": "markdown",
      "source": [
        "We show how the interpolant progressible transforms the Gaussian noise to one of the target samples."
      ]
    },
    {
      "metadata": {
        "id": "kcJWoT5vkorl"
      },
      "cell_type": "code",
      "source": [
        "x_0_dummy = batch['x_0'][0:1, ..., 0]\n",
        "x_1_dummy = batch['x_1'][0:1, ..., 0]\n",
        "# We are not adding extra noise.\n",
        "z_dummy = jnp.zeros_like(x_1_dummy)\n",
        "z_noisy = jax.random.normal(jax.random.PRNGKey(1), shape=x_1_dummy.shape)\n",
        "\n",
        "\n",
        "t_array = jnp.linspace(0, 1, 6)\n",
        "_, axs = plt.subplots(2, 6, figsize=(24, 8))\n",
        "for ii, t in enumerate(t_array):\n",
        "  x_t = interpolant(t[None], x_0_dummy, x_1_dummy, z_dummy)\n",
        "  axs[0, ii].imshow(x_t[0, :, :])\n",
        "  axs[0, ii].set_title(f'Sample from x_{t:\u003c.1f}')\n",
        "  x_t_noisy = interpolant(t[None], x_0_dummy, x_1_dummy, z_noisy)\n",
        "  axs[1, ii].imshow(x_t_noisy[0, :, :])\n",
        "  axs[1, ii].set_title(f'Noisy sample from x_{t:\u003c.1f}')\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "g_BSg52hjTCl"
      },
      "cell_type": "markdown",
      "source": [
        "We consider a generative model that is instantiated by solving the following ODE:\n",
        "$$\\dot{x} = v_{\\theta}(x, t), \\qquad t \\in [0, 1],$$\n",
        "in this case, the model defined below parametrizes $v_{\\theta}(x, t)$."
      ]
    },
    {
      "metadata": {
        "id": "ocXSM5oEnwj7"
      },
      "cell_type": "code",
      "source": [
        "flow_model = reflow_models.RescaledUnet(\n",
        "    out_channels=1,\n",
        "    num_channels=(64, 128),\n",
        "    downsample_ratio=(2, 2),\n",
        "    num_blocks=4,\n",
        "    noise_embed_dim=128,\n",
        "    padding=\"SAME\",\n",
        "    use_attention=True,\n",
        "    use_position_encoding=True,\n",
        "    num_heads=8,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "o3AYca0-nqVv"
      },
      "cell_type": "markdown",
      "source": [
        "We also need to define the distance between the neural network and the speed. In this case we consider the loss:\n",
        "$$|v_{\\theta}(x_t, t) - \\dot{x}_{t}|^2,$$\n",
        "which translates to\n",
        "$$|v_{\\theta}(x_t, t) - ( x_1 - x_0 + \\pi \\sin(2 \\pi t)\\, z)|^2.$$\n",
        "using the fact that $\\dot{x}_{t} = x_1 - x_0 + \\pi \\sin(2 \\pi t)\\, z.$\n",
        "\n",
        "An equivalent loss was already defined in the [``losses``](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/debiasing/stochastic_interpolants/losses.py) module."
      ]
    },
    {
      "metadata": {
        "id": "Jcd9wXZBnpy5"
      },
      "cell_type": "code",
      "source": [
        "loss_stochastic_interpolant = losses.velocity_loss"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "7u9Xc-URnaYi"
      },
      "cell_type": "markdown",
      "source": [
        "Now we have all the required elements to create an instance of ``StochasticInterpolantModel``, which encapsulates all the information at the model level."
      ]
    },
    {
      "metadata": {
        "id": "md1p7pzNnVwW"
      },
      "cell_type": "code",
      "source": [
        "model = models.StochasticInterpolantModel(\n",
        "    input_shape=(\n",
        "        config.input_shapes[0][1],\n",
        "        config.input_shapes[0][2],\n",
        "        config.input_shapes[0][3],\n",
        "    ),  # This must agree with the expected sample shape.\n",
        "    flow_model=flow_model,\n",
        "    # Defines the type of stochastic interpolant.\n",
        "    interpolant=interpolant,\n",
        "    # Defines the type of loss used for the training.\n",
        "    loss_stochastic_interpolant=loss_stochastic_interpolant,\n",
        "    num_eval_cases_per_lvl=8,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "wSuF5VeRkJl2"
      },
      "cell_type": "markdown",
      "source": [
        "### Building the trainer"
      ]
    },
    {
      "metadata": {
        "id": "KaSwHJJuo1f1"
      },
      "cell_type": "markdown",
      "source": [
        "Now, we just need to instantiate the trainer, which contains all the information to run the training loop. This includes the model, the optimizer, and the checkpointer."
      ]
    },
    {
      "metadata": {
        "id": "icw8mWoHWjE8"
      },
      "cell_type": "code",
      "source": [
        "# Defining the trainer.\n",
        "trainer = trainers.StochasticInterpolantTrainer(\n",
        "    model=model,\n",
        "    rng=jax.random.key(config.seed),\n",
        "    optimizer=optimizer,\n",
        "    ema_decay=config.ema_decay,\n",
        ")\n",
        "\n",
        "# Setting up checkpointing.\n",
        "ckpt_options = checkpoint.CheckpointManagerOptions(\n",
        "    save_interval_steps=config.save_interval_steps,\n",
        "    max_to_keep=config.max_checkpoints_to_keep,\n",
        ")\n",
        "\n",
        "# Sets up the working directory.\n",
        "workdir = \"/content\"  # typical current position in Colab."
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "r0gC0ChUeh5c"
      },
      "cell_type": "code",
      "source": [
        "### If you need to remove the checkpoint to start from scratch.\n",
        "!rm -Rf /content/checkpoints"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "WWxXqMZnhE3d"
      },
      "cell_type": "markdown",
      "source": [
        "### Running the training loop.\n",
        "\n",
        "We run the training loop.\n",
        "\n",
        "Here the seek to solve the problem\n",
        "\n",
        "$$ \\min_{\\theta} \\mathbb{E}_{z \\sim N(0, \\sigma^2 I)}\\mathbb{E}_{t \\sim U[0, 1]} \\mathbb{E}_{(x_0, x_1) \\in \\mu_0 \\otimes \\mu_1} \\left \\| \\dot{x}_t - v_{\\theta}(x_t, t)  \\right \\|^2,$$\n",
        "where $x_t = t x_1 + (1-t) x_0 + \\sin^2(\\pi t) z$, $\\mu_0$ distribution of blurred MNIST digits and $\\mu_1$ is the distribution of the original MNIST digits.\n",
        "\n",
        "\n",
        "This loss can be further simplified as\n",
        "$$ \\min_{\\theta} \\mathbb{E}_{z \\sim N(0, \\sigma^2 I)} \\mathbb{E}_{t \\sim U[0, 1]} \\mathbb{E}_{(x_0, x_1) \\in \\mu_0 \\otimes \\mu_1}   | v_{\\theta}(x_t, t)|^2  - 2 ( x_1 - x_0 + \\pi \\sin(2\\pi t)\\,z) \\cdot  v_{\\theta}(x_t, t),$$\n",
        "using the fact that $\\dot{x}_t = x_1 - x_0 + \\pi \\sin(2\\pi t)\\,z $ and that $\\dot{x}_t$ is independent of $\\theta$.\n",
        "\n",
        "Note the full training step it takes around 15-20 mins in a TPU v6e (Trillium).\n"
      ]
    },
    {
      "metadata": {
        "id": "m4MGSJeXaWSe"
      },
      "cell_type": "code",
      "source": [
        "# Run training loop.\n",
        "\n",
        "train.run(\n",
        "    train_dataloader=train_dataloader,\n",
        "    trainer=trainer,\n",
        "    workdir=workdir,\n",
        "    total_train_steps=config.num_train_steps,\n",
        "    metric_aggregation_steps=config.metric_aggregation_steps,  # 30\n",
        "    eval_dataloader=eval_dataloader,\n",
        "    eval_every_steps=config.eval_every_steps,\n",
        "    num_batches_per_eval=config.num_batches_per_eval,\n",
        "    metric_writer=metric_writers.create_default_writer(\n",
        "        workdir, asynchronous=False\n",
        "    ),\n",
        "    callbacks=(\n",
        "        callbacks.TqdmProgressBar(\n",
        "            total_train_steps=config.num_train_steps,\n",
        "            train_monitors=(\"train_loss\",),\n",
        "        ),\n",
        "        # This callback saves model checkpoint periodically.\n",
        "        callbacks.TrainStateCheckpoint(\n",
        "            base_dir=workdir,\n",
        "            options=ckpt_options,\n",
        "        ),\n",
        "        # TODO add a plot callback.\n",
        "    ),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "F5c3-q7FkZWc"
      },
      "cell_type": "markdown",
      "source": [
        "# Running Inference"
      ]
    },
    {
      "metadata": {
        "id": "41SUZuGgmCn7"
      },
      "cell_type": "markdown",
      "source": [
        "Loading extra libraries for running inference."
      ]
    },
    {
      "metadata": {
        "id": "4oYyp_5Nl_n1"
      },
      "cell_type": "code",
      "source": [
        "import functools\n",
        "from swirl_dynamics.lib.solvers import ode as ode_solvers\n",
        "from tqdm import tqdm"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "uc8IMrsclUxQ"
      },
      "cell_type": "markdown",
      "source": [
        "Define the dataloader to run inference."
      ]
    },
    {
      "metadata": {
        "id": "uCC3H1bEdMTF"
      },
      "cell_type": "code",
      "source": [
        "test_dataloader = get_mnist_dataset(\n",
        "    split=\"test\", batch_size=config.batch_size_eval, repeat=False\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2QyDzd5LlwIq"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the last trained model and define the dynamics."
      ]
    },
    {
      "metadata": {
        "id": "WK2Y1RC2lvKQ"
      },
      "cell_type": "code",
      "source": [
        "trained_state = trainers.TrainState.restore_from_orbax_ckpt(\n",
        "    f\"{workdir}/checkpoints\", step=None\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Udq4tYKKl4vt"
      },
      "cell_type": "code",
      "source": [
        "latent_dynamics_fn = ode_solvers.nn_module_to_dynamics(\n",
        "    model.flow_model,\n",
        "    autonomous=False,\n",
        "    is_training=False,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "AU8-AFJep_5X"
      },
      "cell_type": "markdown",
      "source": [
        "We define the ODE solver, (here Runge-Kutta 4th order), and other details such as the number of steps."
      ]
    },
    {
      "metadata": {
        "id": "kgweIuX4l9GF"
      },
      "cell_type": "code",
      "source": [
        "num_sampling_steps = 128\n",
        "\n",
        "integrator = ode_solvers.RungeKutta4()\n",
        "integrate_fn = functools.partial(\n",
        "    integrator,\n",
        "    latent_dynamics_fn,\n",
        "    tspan=jnp.arange(0.0, 1.0, 1.0 / num_sampling_steps),\n",
        "    params=trained_state.model_variables,\n",
        ")\n",
        "\n",
        "integrate_fn_jit = jax.jit(integrate_fn)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6hf0XVLJqH_F"
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(test_dataloader))\n",
        "print(f\"Shape of the x_0 condition, {batch['x_0'].shape}\")\n",
        "out_put = integrate_fn_jit(batch[\"x_0\"])\n",
        "print(f\"Shape of the generated x_1 {out_put.shape}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "a19KqEGISluG"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow(out_put[-1, 0, :, :, 0])\n",
        "plt.title(\"Sample from target distribution x_1\")\n",
        "plt.xticks([])\n",
        "plt.yticks([]);"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "beDn7J_9qRjd"
      },
      "cell_type": "code",
      "source": [
        "num_plots = 6\n",
        "idx_batch = 10\n",
        "fig, axs = plt.subplots(1, num_plots, figsize=(num_plots * 4, 4))\n",
        "num_samples = out_put.shape[0]\n",
        "idx_samples = np.linspace(0, num_samples - 1, num_plots).astype(int)\n",
        "for ii, idx in enumerate(idx_samples):\n",
        "  axs[ii].imshow(out_put[idx, idx_batch, :, :, 0])\n",
        "  axs[ii].set_title(f\"x_t for t= {(idx/num_samples):\u003c.2f}\")\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "SJzKJdGFnESD"
      },
      "cell_type": "markdown",
      "source": [
        "### Running Inference Loop.\n",
        "\n",
        "This may take a non-negligible amount of time."
      ]
    },
    {
      "metadata": {
        "id": "v2vqQkvUmrk7"
      },
      "cell_type": "code",
      "source": [
        "input_list = []\n",
        "output_list = []\n",
        "ground_truth_list = []\n",
        "\n",
        "for ii, batch in tqdm(enumerate(test_dataloader)):\n",
        "  input_list.append(batch[\"x_0\"])\n",
        "  ground_truth_list.append(batch[\"x_1\"])\n",
        "\n",
        "  output = np.array(\n",
        "      integrate_fn_jit(batch[\"x_0\"])[-1].reshape(\n",
        "          (-1, config.input_shapes[1][1], config.input_shapes[1][2])\n",
        "      )\n",
        "  )\n",
        "  output_list.append(output)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "j4bBlRPeYz-P"
      },
      "cell_type": "code",
      "source": [
        "input_array = np.concatenate(input_list, axis=0)\n",
        "print(f\"Shape of the input array: {input_array.shape}\")\n",
        "output_array = np.concatenate(output_list, axis=0)\n",
        "print(f\"Shape of the output array: {output_array.shape}\")\n",
        "ground_truth_array = np.concatenate(ground_truth_list, axis=0)\n",
        "print(f\"Shape of the ground_truth array: {ground_truth_array.shape}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "INuCK8YBY7iq"
      },
      "cell_type": "code",
      "source": [
        "num_plots = 6\n",
        "fig, axs = plt.subplots(3, num_plots, figsize=(num_plots * 4, 12))\n",
        "num_samples = output_array.shape[0]\n",
        "idx_samples = np.linspace(0, num_samples - 1, num_plots).astype(int)\n",
        "for ii, idx in enumerate(idx_samples):\n",
        "  axs[0, ii].imshow(input_array[idx, :, :])\n",
        "  axs[0, ii].set_title(f\"Input sample number: {idx}\")\n",
        "  axs[1, ii].imshow(output_array[idx, :, :])\n",
        "  axs[1, ii].set_title(f\"Output sample number: {idx}\")\n",
        "  axs[2, ii].imshow(ground_truth_array[idx, :, :])\n",
        "  axs[2, ii].set_title(f\"Ground truth sample number: {idx}\")\n",
        "\n",
        "  # Removes ticks.\n",
        "  axs[0, ii].set_xticks([])\n",
        "  axs[0, ii].set_yticks([])\n",
        "  axs[1, ii].set_xticks([])\n",
        "  axs[1, ii].set_yticks([])\n",
        "  axs[2, ii].set_xticks([])\n",
        "  axs[2, ii].set_yticks([])\n",
        "\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V6E1",
      "last_runtime": {
        "build_target": "//experimental/users/lzepedanunez:notebook_minimal",
        "kind": "private"
      },
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1IfkjhYe61a8LIsbUBgmh1_rJ6PU5R48x",
          "timestamp": 1748968282742
        },
        {
          "file_id": "1oURobt7ZEI0TWTz4QkBkvwlY8QU3GX44",
          "timestamp": 1748839000581
        },
        {
          "file_id": "https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/debiasing/stochastic_interpolants/colabs/demo_stochastic_interpolants.ipynb",
          "timestamp": 1748583300635
        },
        {
          "file_id": "1EMyBTRxEvF-XHnKQK3HNgWvqKYNpzAfi",
          "timestamp": 1745815061659
        },
        {
          "file_id": "1LaxpTzo8aucfAAj5eI8buNWY4pDI_T6d",
          "timestamp": 1745555288947
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
