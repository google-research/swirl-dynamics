{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJLuCB-HVGVN"
      },
      "source": [
        "# Tutorial on Diffusion Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0s2Eh6sVMwM"
      },
      "source": [
        "This tutorial seeks to provide a succinct introduction to diffusion models in an interactive fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41RRUE_3VWFE"
      },
      "source": [
        "The main task of diffusion models is to generate samples, $x$, from an *unknown* distribution $p(x)$, from which we only have access to some of its samples $\\{x_i \\}_{i=0}^n \\sim p(x)$. Here $x$ can represent a myriad of different elements, such as images, videos, words, etc, and it is high-dimensional, i.e.,  $x_i \\in \\mathbb{R}^d$, where $d \u003e\u003e 1$.\n",
        "\n",
        "As such, the task of sampling from this *unknown* distribution $p(x)$ is two-fold:\n",
        "\n",
        "1.   estimate $p(x)$, and\n",
        "2.   sample from $p(x)$.\n",
        "\n",
        "In this tutorial we will describe how diffusion models are able to perform these tasks.\n",
        "\n",
        "As we assume that the reader does not have prior knowledge on sampling methods. We will divide this tutorial in two sections:\n",
        "\n",
        "\n",
        "1.    we assume that $p(x)$ is known (up to a constant), and we present a couple of algorithms showcasing how to **draw** samples from this distribution, and\n",
        "2.    we will show how to modify these algorithms to also **estimate** $p(x)$ from its **samples**.\n",
        "\n",
        "For the first stage we will use the paradigm of generating sampling from an easy-to-sample distribution and we will transform it to samples of the target distribution in the one-dimensional case. Then we will extend this idea to higher dimension where the transformation will be a composition of a large amount of small transformations (usually driven by a Stochastic Differential Equation associated with a *diffusion process*, hence the origin of the name).\n",
        "\n",
        "Then we will use a similar approach but in two-directions. We will consider a noising process which transforms any samples of the target distribution to Gaussian noise, by using a sequence of small transformation that progressively add noise to the samples. Then we will revert this process, so any Gaussian noise can be reverted back to a sample from the target distribution by a sequence of transformation that progressively **denoise**, or extract, the noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "NAazBOEcyPfg"
      },
      "outputs": [],
      "source": [
        "# @title Imports\n",
        "from functools import partial\n",
        "from typing import Any, Callable\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.graph_objects as go\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "\n",
        "PyTree = Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YWcNr_WXrr"
      },
      "source": [
        "### Sampling from $p(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Bawyb5W7Oq"
      },
      "source": [
        "For simplicity we will start with a simpler problem. Suppose that $x \\in \\mathbb{R}$ and that $p(x)$ is know.\n",
        "\n",
        "Our simplified task is to sample from this distribution for an arbitrary $p$. The methodology introduced here will be similar for more complex tasks. In a nutshell the methodology has two steps:\n",
        "\n",
        "1.   obtain samples from a distribution that it is easy-to-sample, e.g, a uniform distribution, and\n",
        "2.   transform those samples, so that the results follow the target distribution.\n",
        "\n",
        "We show below how this usually done in practice for simpler problems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P24e-8A4_Zap"
      },
      "source": [
        "### Inverse Transform Sampling\n",
        "\n",
        "One of the simplest algorithms for sampling following a known one-dimensional distribution $p$, is the so called Inverse Transform Sampling. This technique stems from the fact that for any continuous one-dimensional distribution $p$, its cumulative density function\n",
        "$$\n",
        "    F_{X}(x) = \\int_{-\\infty}^x p(s) ds\n",
        "$$\n",
        "induces a uniform random variable in $[0, 1]$.\n",
        "\n",
        "In other words, if a random variable $X \\sim p$ then $U := F_{X}(X)$ is an uniform random variable. I.e., $U \\sim \\mathbb{U}[0,1]$. Now, if $F_X$ in invertible, then we can recover $X$ by transforming $U$ following $X = F_{X}^{-1}(U)$.\n",
        "\n",
        "This observation is useful to generate samples from $X$:\n",
        "\n",
        "1.   one draws samples $u_i \\sim U$, which can be easily done with any pseudo-random number generator,\n",
        "2.   and then transform them through $F_X^{-1}$, i.e., $x_i = F_X^{-1}(u_i)$ are such that $x_i \\sim X$.\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "In the code below we show how this can be used for sampling from two different distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_NNUBgsjJAw"
      },
      "source": [
        "**Remark:**\n",
        "This stems from the fact that if $U \\sim \\mathbb{U}[0, 1]$ and $F: \\mathbb{R} \\rightarrow [0, 1]$ is a positive, weakly monotonic, and right-continuous function then $X = F^{-1}(U)$ has $F$ as its CDF.\n",
        "\n",
        "This statement can be proved with a simple calculation: where the CDF of the random variable $X= F^{-1}(U)$ is given by\n",
        "$$\n",
        "    F_X(x) := \\mathbb{P}(X \\leq x) = \\mathbb{P}(F^{-1}(U) \\leq x) = \\mathbb{P}(U \\leq F(x)) = F(x),\n",
        "$$\n",
        "where we used the fact that as $F$ is right-continuous so we have the following set equality $\\{u: F^{-1}(u) \\leq x \\} = \\{u: u \\leq F(x) \\}$, and the fact that if $u\\leq 1$, then $\\mathbb{P}(U \\leq u) = u$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Mvg0tBJyMVL"
      },
      "source": [
        "Here we demonstrate this approach by sampling from the target distribution\n",
        "$$p(x) = \\frac{\\pi}{2} \\sin (\\pi x)$$\n",
        "where\n",
        "$$F_X(x) =  \\frac{1}{2}(1 -\\cos(\\pi x)), $$\n",
        "and a quick computation shows that\n",
        "$$F^{-1}_X(u) = \\frac{\\arccos(1 - 2u)}{\\pi}.$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0_OpD5DW6yX"
      },
      "outputs": [],
      "source": [
        "# Samples u from an uniform distribution.\n",
        "key = jax.random.PRNGKey(37)\n",
        "u = jax.random.uniform(key, (20000,))\n",
        "\n",
        "# Transforms the samples to x, using X = F_{X}^{-1}(U).\n",
        "x = jnp.arccos(1 - 2*u)/jnp.pi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGAFQR_q4Fu1"
      },
      "source": [
        "Now we plot the histograms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oVeRt3aVDzb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(layout=\"constrained\", figsize=(10, 4))\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), layout=\"constrained\")\n",
        "\n",
        "counts, bins = jnp.histogram(u, bins=50, density=True)\n",
        "ax[0].hist(bins[:-1], bins, weights=counts, label=\"histogram\")\n",
        "ax[0].set_title(\"Uniform distribution\")\n",
        "\n",
        "counts, bins = jnp.histogram(x, bins=50, density=True)\n",
        "ax[1].hist(bins[:-1], bins, weights=counts, label=\"histogram\")\n",
        "ax[1].plot(\n",
        "    bins,\n",
        "    0.5 * jnp.pi * jnp.sin(jnp.pi * bins),\n",
        "    linewidth=4.5,\n",
        "    color=\"orange\",\n",
        "    label=\"p(x)\",\n",
        ")\n",
        "ax[1].set_title(\"Target Distribution\")\n",
        "ax[1].legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvRFR6Y9jmTv"
      },
      "source": [
        " In the left we have the histogram of the uniform distribution and in the right we have the histogram of the transformed samples which approximates the target distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twp01c8S4hRm"
      },
      "source": [
        "We can repeat this operation for distributions without a compact support, e.g., we can consider\n",
        "$$ p(x) = \\frac{e^{\\sqrt{x}}}{2\\sqrt{x}},$$\n",
        "with CDF\n",
        "$$F_{X}(x) = 1 - e^{-\\sqrt{x}},$$\n",
        "and inverse\n",
        "$$F_{X}^{-1}(u) = \\left ( \\log(1 - u)\\right)^2$$.\n",
        "\n",
        "In this case the density will decrease rapidly as $x$ increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwLDTp5u4goL"
      },
      "outputs": [],
      "source": [
        "# Samples u from an uniform distribution.\n",
        "key = jax.random.PRNGKey(37)\n",
        "u = jax.random.uniform(key, (1000000,))\n",
        "\n",
        "# Transforms the samples to x, using X = F_{X}^{-1}(U).\n",
        "x = jnp.square(jnp.log(1 - u))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqONcraxzTgZ"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(layout=\"constrained\", figsize=(10, 4))\n",
        "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4), layout=\"constrained\")\n",
        "\n",
        "counts, bins = jnp.histogram(u, bins=300, density=True)\n",
        "ax[0].hist(bins[:-1], bins, weights=counts, label=\"histogram\")\n",
        "ax[0].set_title(\"Uniform distribution\")\n",
        "\n",
        "counts, bins = jnp.histogram(x, bins=300, density=True)\n",
        "ax[1].hist(bins[:-1], bins, weights=counts, label=\"histogram\")\n",
        "ax[1].semilogy(\n",
        "    bins[1:],\n",
        "    0.5 * jnp.exp(-jnp.sqrt(bins[1:])) / jnp.sqrt(bins[1:] + 0.01),\n",
        "    color=\"orange\",\n",
        "    linewidth=4.5,\n",
        "    label=\"p(x)\",\n",
        ")\n",
        "ax[1].set_title(\"Target Distribution (in semilog scale)\")\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aLf61ZRjsOQ"
      },
      "source": [
        "Here we can also observe how the uniform samples are transformed to the target distribution, which decays rapidily as mentioned above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-EHyA_dFQ6K"
      },
      "source": [
        "### Langevin Sampler\n",
        "\n",
        "The Inverse Transformation Sampler is simple and efficient; however, it is only restricted to one-dimensional distributions. To handle higher dimension many different approaches have been proposed in the literature.\n",
        "\n",
        "Among them the Langevin sampler is perhaps one of the most related to diffusion models. The Langevin sampler leverages properties of the Langevin equation at equilibrium, i.e., in the asymptotic limit as $t \\rightarrow \\infty$.\n",
        "\n",
        "The [overdamped Langevin equation](https://en.wikipedia.org/wiki/Brownian_dynamics) is usually written as\n",
        "$$\n",
        "    dX_{\\tau} = -\\nabla V(X_{\\tau}) d\\tau + \\sqrt{2} \\sigma dW_{\\tau},\n",
        "$$\n",
        "where $V: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{+}$ is a potential function and $dW_{\\tau}$ is a [Wiener process](https://en.wikipedia.org/wiki/Wiener_process), and with initial distribution $X_0 \\sim p_o$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqZRt6GWj5nI"
      },
      "source": [
        "#### Physical meaning of the components\n",
        "\n",
        "We briefly explain the physical meaning of each term of the overdamped Langevin equation. On the one hand, if there is no stochastic term ($\\sigma = 0$), the equation becomes just a gradient flow, i.e., it seeks to push $X_{\\tau}$ to the areas that minimizes the potential. E.g., if we consider the quadratic potential centered at the origin $V(x) := \\frac{\\|x \\|^2}{2}$, with minimizer in the origin, thus $-\\nabla V$ always points towards the origin. In this case, the gradient flow given by\n",
        "$$ \\dot{X_{\\tau}} =  -\\nabla V(X_{\\tau})$$\n",
        "will transport $X_{\\tau}$ to the origin as $\\tau \\rightarrow \\infty$.\n",
        "On the other hand, if there is only the stochastic term ($V$ is constant),  the equation becomes a random walk:\n",
        "$$dX_{\\tau} = \\sqrt{2} \\sigma dW_{\\tau},$$\n",
        "which will randomly explore the full space.\n",
        "\n",
        "As such, there are two competing forces: one is the potential that seeks to push $X_{\\tau}$ to the zones of lowest potential, and there is the stochastic part that injects energy to the system randomly. If $V$ has more than one local minima, then the stochastic part would help the trajectory to \"jump\" from one basin of attraction to another one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDagp27wklXF"
      },
      "source": [
        "#### Equilibrium\n",
        "\n",
        "The main reason to use this equation for transforming samples as we did in the Inverse Transform Sampler lies in the expression for its limiting distribution.\n",
        "Basically, for any initial distribution if we solve the Langevin equation for long enough, the trajectories will follow a limiting distribution which **only** depends on $V$ and $\\sigma$.\n",
        "\n",
        "We bypass the computation, but we show that for $\\tau \u003e\u003e 1$ we have that $X_{\\tau} \\sim p_{\\infty}$, where $p_{\\infty}$ is completely determined by the potential and the noise level, and it is given by\n",
        "$$p_{\\infty}(x) = c e^{-V(x)/\\sigma^2},$$\n",
        "where $c$ is a normalizing constant.\n",
        "\n",
        "This means that we can manipulate $V$ and $\\sigma$ so the limiting distribution is our target distribution $p$. Thus by letting $ V = \\log p $ and $\\sigma = 1$ we have that\n",
        "$$\n",
        "    dX_{\\tau} = -\\nabla \\log p(X_{\\tau}) d\\tau + \\sqrt{2}dW_{\\tau},\n",
        "$$\n",
        "will have as limiting disitribution\n",
        "$$p_{\\infty}(x) = c e^{-V(x)/\\sigma^2} = c p(x) = p(x),$$\n",
        "which is exactly our target distribution.\n",
        "\n",
        "Then you can use the Langevin equation to sample from an easy-to-sample distribution, and use that sample as an initial condition to solve the SDE, thus evolving the trajectory until the solution has reached the limiting distribution (e.g., at the time horizon $T \u003e\u003e 1$). The resulting state will be a sample of the target distribution.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqGgZcLKkneW"
      },
      "source": [
        "**Remarks**\n",
        "\n",
        "The function $\\nabla \\log p(X_{\\tau})$ is usually called the **score function**. As we are considering the derivate of a logarithm, we have that this function is agnostic to normalizations. I.e., suppose that you have $q(x) = c p(x)$, then\n",
        "$$\\nabla \\log q(x) = \\nabla \\log c p(x) =  \\nabla \\log c + \\nabla \\log p(x) = \\nabla \\log p(x),$$\n",
        "as $\\nabla \\log c = 0$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xn5D54f3IMas"
      },
      "source": [
        "#### Euler-Maruyama Solver\n",
        "\n",
        "Here we consider a more general SDE which can be written as\n",
        "\n",
        "$$\n",
        "    dX_{\\tau} = f(X_{\\tau}, \\tau) d\\tau + g(\\tau) dW_{\\tau},\n",
        "$$\n",
        "the main difference is that we allow the drift and the diffusion term to be time-dependent. Now in what follows we implement the Euler-Maruyama solver in Jax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtOKdRwg49w9"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=(2, 3, 4, 5, 6))\n",
        "def euler_maruyama_solver(key:jax.Array,\n",
        "                x_0: jax.Array,\n",
        "                f: Callable[[jax.Array, jax.Array], jax.Array],\n",
        "                g: Callable[[jax.Array], jax.Array],\n",
        "                dt: float=0.001,\n",
        "                t_final: float=1.,\n",
        "                return_trajectory: bool=False) -\u003e jax.Array:\n",
        "    \"\"\"Solver for the forward equation using Euler-Maruyama.\n",
        "\n",
        "    Args:\n",
        "        key: Random seed.\n",
        "        x_0: Initial condition.\n",
        "        f: Drift coefficient.\n",
        "        g: Diffusion coefficient.\n",
        "        dt: Time step.\n",
        "        t_final: Time horizon.\n",
        "\n",
        "    Returns:\n",
        "        Sample from the target distribution.\n",
        "    \"\"\"\n",
        "    def time_step(carry: tuple[jax.Array, jax.Array], params_time):\n",
        "        key, x = carry\n",
        "        t, dt = params_time\n",
        "        key, subkey = jax.random.split(key)\n",
        "        # Now we run the kernel using Euler-Maruyama\n",
        "        diff = g(t)\n",
        "        deltaW = jax.random.normal(key=subkey, shape=(x.shape))\n",
        "        drift = f(x, t)\n",
        "        x = x + dt*drift + jnp.sqrt(dt)*diff*deltaW\n",
        "        return (key, x), (x)\n",
        "\n",
        "    key, _ = jax.random.split(key)\n",
        "    n_steps = int(1/dt)\n",
        "    time_steps = t_final * jnp.arange(1, n_steps)/(n_steps-1)\n",
        "    # We allow for non-uniform discretization in time.\n",
        "    delta_t = time_steps[1:] - time_steps[:-1]\n",
        "    params_time = jnp.stack([time_steps[:-1], delta_t], axis=1)\n",
        "    # Initial condition.\n",
        "    carry = (key, x_0)\n",
        "    (_, samples), trajectory = jax.lax.scan(time_step, carry, params_time)\n",
        "    # Whether to return the full trajectory of just the last element.\n",
        "    if return_trajectory:\n",
        "      return trajectory\n",
        "    else:\n",
        "      return samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K1HTi2l6NMD"
      },
      "source": [
        "Thus we can manipulate $f$ and $g$ appropiately following the Langevin equation above. Here we define a density centered in the sphere of radious 1.\n",
        "\n",
        "$$p(x):= c \\cdot e^{-| \\| x\\|^2 - 1 |}.$$\n",
        "\n",
        "Again, we don't need to prescribe the normalization constant.\n",
        "\n",
        "Here we also leverage the jax transformation to compute the the score function of $p$,\n",
        "$$\\nabla_x \\log p(x),$$\n",
        "given that $f$ is supposed to be also time dependent so we add a dummy variable in the definition of the functions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE1e3hqv3M1n"
      },
      "outputs": [],
      "source": [
        "# Function proportional to the target distribution.\n",
        "def p(x, t):\n",
        "  return jnp.exp(-jnp.abs(jnp.sum(jnp.square(x)) - 1.)) + 0.*jnp.sum(t)\n",
        "\n",
        "# Noise level.\n",
        "def sigma(t):\n",
        "  return 0.*t + jnp.sqrt(2)*0.001\n",
        "\n",
        "logp = lambda x,t: jnp.log(p(x,t))\n",
        "\n",
        "# Compute the gradient of the log-likely hood or score function.\n",
        "nablaV = jax.grad(logp, argnums=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWb6Z1p18xDh"
      },
      "source": [
        "We define the dimension and the number of samples to be drawn and we vectorize the sampler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qquxJpJuqM-t"
      },
      "outputs": [],
      "source": [
        "# Dimension of the samples.\n",
        "d = 3\n",
        "# Defines the number of samples.\n",
        "n_samples = 1000\n",
        "\n",
        "# We vectorize the sampler to draw several samples simultaneously.\n",
        "sampler_vectorized = jax.vmap(\n",
        "    partial(\n",
        "        euler_maruyama_solver, f=nablaV, g=sigma, dt=0.001, t_final=40\n",
        "        ),\n",
        "    in_axes=(0, 0), out_axes=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SKMi55AqNcj"
      },
      "source": [
        "Now we perform the same process as before:\n",
        "\n",
        "1.   we sample from an easy-to-sample distribution, here is a simple Normal distribution, and\n",
        "2.   we transform the samples using them as initial condition to the SDE solver.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtAD7e3i8pp7"
      },
      "outputs": [],
      "source": [
        "# It creates the array of random seeds.\n",
        "key_array = jax.random.split(key, n_samples)\n",
        "# We sample from the initial condition.\n",
        "x_0 = jax.random.normal(key, shape=(n_samples, d))\n",
        "# We solve the Langevin equation in vectorized form.\n",
        "samples = sampler_vectorized(key_array, x_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhv_dbbH6vz_"
      },
      "source": [
        "We can plot the samples, which, unsurprisingly, concentrate around the sphere of radius 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2dZJroSyzbV"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Scatter3d(x=samples[:,0], y=samples[:,1], z=samples[:,2],\n",
        "                                   mode='markers',\n",
        "                                   marker=dict(size=3,opacity=0.8)),])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kstkdD2U7rHz"
      },
      "source": [
        "Here we also observe that the starting points (in red) were taken from a Gaussian disctribution, and they were transformed to the samples of the target one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAueolCWtpsQ"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Scatter3d(x=samples[:,0], y=samples[:,1], z=samples[:,2],\n",
        "                                   mode='markers',\n",
        "                             marker=dict(size=3,opacity=0.8)),\n",
        "                go.Scatter3d(x=x_0[:,0], y=x_0[:,1], z=x_0[:,2],\n",
        "                                   mode='markers',\n",
        "                             marker=dict(size=3,opacity=0.8)),])\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOSgL4ls-IS3"
      },
      "source": [
        "You can easily choose a different target distribution, for example using an ellipsoid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CyqUoWe8qkG"
      },
      "outputs": [],
      "source": [
        "A = jnp.array([[1.5, 0., 0.],[0., 0.75, 0.],[0., 0., 1.]])\n",
        "def p(x, t):\n",
        "  return jnp.exp(-jnp.abs(jnp.sum(jnp.square(A @ x)) - 4.)) + 0.*jnp.sum(t)\n",
        "\n",
        "logp = lambda x,t: jnp.log(p(x,t))\n",
        "\n",
        "nablaV = jax.grad(logp, argnums=0)\n",
        "\n",
        "# As we changed the density and therefore the potential we also change the\n",
        "# equation.\n",
        "sampler_vectorized = jax.vmap(\n",
        "    partial(\n",
        "        euler_maruyama_solver, f=nablaV, g=sigma, dt=0.001, t_final=10\n",
        "        ),\n",
        "    in_axes=(0, 0), out_axes=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRDZ-hagB_fI"
      },
      "outputs": [],
      "source": [
        "# Defines the number of samples.\n",
        "n_samples = 2000\n",
        "# It creates the array of random seeds.\n",
        "key_array = jax.random.split(key, n_samples)\n",
        "# We sample from the initial condition.\n",
        "x_0 = jax.random.normal(key, shape=(n_samples, d))\n",
        "# Sampling by solving the Langevin equation.\n",
        "samples = sampler_vectorized(key_array, x_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BzLl6XSE-g1K"
      },
      "source": [
        "And we can plot the starting samples of the Langevin equation and the target samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXrY_eby-ZrS"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Scatter3d(x=samples[:,0], y=samples[:,1], z=samples[:,2],\n",
        "                                   mode='markers',\n",
        "                             marker=dict(size=3,opacity=0.8)),\n",
        "                go.Scatter3d(x=x_0[:,0], y=x_0[:,1], z=x_0[:,2],\n",
        "                                   mode='markers',\n",
        "                             marker=dict(size=3,opacity=0.4)),])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROdKaqmx8py0"
      },
      "source": [
        "## Learning the distribution.\n",
        "\n",
        "Thus far, we have addressed the initial question: how to sample from a known target distribution. To achieve this, we employed the paradigm of transforming samples from an easy-to-sample distribution into samples of the target distribution.\n",
        "\n",
        "The subsequent question arises: what if the target distribution is *unknown*? Fortunately, diffusion models offer an efficient and elegant solution by learning the *score function* associated to the distribution.\n",
        "\n",
        "Diffusion models exploit the same principles used for generating samples with the Langevin solver discussed earlier.\n",
        "\n",
        "We started with a arbitrary but easy-to-sample distribution, and by evolving the SDE we samples of the limiting distribution, which is equal to the target distribution by manipulating the drift and diffusion coefficients. Here, we do **start** from the **target distribution** and we manipulate the SDE so the limiting distribution is the simple-to-sample distribution. Then we seek to reverse this process: we sample from the easy-to-sample distribution and the **evolve** the SDE **backwards** to obtain a sample of the target one. \n",
        "\n",
        "Now, instead of integrating for a long time, we leverage the fact that drift and diffusion coefficients are time dependent to increase the noise much faster. In particular, we choose the following functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwQnUNfmEdAG"
      },
      "outputs": [],
      "source": [
        "beta_min = 0.001\n",
        "beta_max = 500\n",
        "\n",
        "def beta(t: jax.Array) -\u003e jax.Array:\n",
        "  return beta_min + t*(beta_max - beta_min)\n",
        "\n",
        "def f(x: jax.Array, t: jax.Array) -\u003e jax.Array:\n",
        "  return -0.5*beta(t)*x\n",
        "\n",
        "def g(t: jax.Array) -\u003e jax.Array:\n",
        "  return jnp.sqrt(beta(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syqEdh3UP_ay"
      },
      "source": [
        "Let's see an example of this. We load the picture of a dog, and then we will run a similar SDE but using the image of the dog as an initial condition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iOKkFvnDQ3_"
      },
      "outputs": [],
      "source": [
        "data_set = tfds.load('stanford_dogs', split='train')\n",
        "dog = next(iter(data_set))\n",
        "dog_image = np.array(dog['image'])[::2,::2]\n",
        "plt.imshow(dog_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoh6lGYUQLu-"
      },
      "source": [
        "Now we define a slightly different implementation of the solver, as we want to obtain the full trajectory of the noising process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZgCl5x7EoV7"
      },
      "outputs": [],
      "source": [
        "# We take the image of the dog an initial condition.\n",
        "x_0 = dog_image.astype(np.float64).reshape((-1,)) / 256.\n",
        "rng = jax.random.PRNGKey(0)\n",
        "# Solves the forward SDE.\n",
        "trajectory = euler_maruyama_solver(rng, x_0, f, g, return_trajectory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKbjteM3Qx9R"
      },
      "source": [
        "Now we can plot how our image of the dog becomes increasingly noised. In this case we plot only one RGB channel to make things easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgaVU6j5E6X2"
      },
      "outputs": [],
      "source": [
        "traj_image = trajectory.reshape((-1, *dog_image.shape))\n",
        "fig, axes = plt.subplots(1, 6, figsize=(24, 8))  # Adjust figsize for desired dimensions\n",
        "idx = [0, 20, 40, 60, 80, 97]\n",
        "for i, ax in enumerate(axes):\n",
        "    ax.imshow(traj_image[idx[i],:,:])\n",
        "    ax.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS5KbbeIJMW1"
      },
      "source": [
        "As we can see the picture of the dog become incresingly noised as we progress **forward** in the simulation of the SDE. Now, diffusion models seek to reverse this process. Fortunately, it is known that for the SDE\n",
        "\n",
        "$$\n",
        " dX_{\\tau} = f(X_{\\tau}, \\tau) d\\tau + g(\\tau) dW_{\\tau},\n",
        "$$\n",
        "\n",
        "can be reversed in time, by solving the following SDE backwards\n",
        "$$\n",
        " dX_{\\tau} = \\left ( f(X_{\\tau}, \\tau) - \\nabla_x \\log p(X_{\\tau}, \\tau) \\right ) d\\tau + g(\\tau) d\\bar{W}_{\\tau},\n",
        "$$\n",
        "where $d\\bar{W}_{\\tau}$ is a backward Wiener process, and $p(X_{\\tau}, \\tau)$ is the marginal distribution of the full process at time $\\tau$, i.e., $X_{\\tau} \\sim  p(X_{\\tau}, \\tau)$.\n",
        "\n",
        "The question becomes on how to compute the score function. Here is where the key component of diffusion models comes to light: the Tweedie's formula. If\n",
        "\n",
        "$$X_{\\tau} = s(\\tau) X_{0} + \\varepsilon_{\\tau}\\,,\\quad\\text{with}\\quad \\varepsilon_{\\tau}\\sim N(0,\\sigma^{2}(\\tau)I)\\,.$$\n",
        "\n",
        "then we have that\n",
        "\n",
        "$$\\mathbb{E}[X_{0}|X_{\\tau}] = \\frac{X_{\\tau} + \\sigma^{2}(\\tau)\\nabla_{x}\\log{p(X_{\\tau}, \\tau)}}{s(\\tau)} \\quad\\Rightarrow\\quad\\nabla_{x}\\log{p(X_{\\tau}, \\tau)} = \\frac{s(\\tau)\\mathbb{E}[X_{0}|X_{\\tau}] - X_{\\tau}}{\\sigma^{2}(\\tau)}\\,.$$\n",
        "\n",
        "where $\\mathbb{E}[X_{0}|X_{\\tau}]$ is interpreted as denoiser. It takes the noisy image $X_{\\tau}$ and computes the maximum likelyhood of the original image $X_0$. Now, we can approximate this denoiser by a neural network, such that\n",
        "$$D_{\\theta} (x + \\sigma \\varepsilon, \\sigma) \\approx x$$.\n",
        "\n",
        "This allows us to build this score function by leveraging the denoiser, which is parametrized by a neural network, and then we can solve the backwards SDE introduced above to sample from the target distribution.\n",
        "\n",
        "We will consider the data that was generated before as our target distribution. For an example using images the reader can take a look at this [colab](https://github.com/google-research/swirl_dynamics/projects/probabilistic_diffusion/colabs/demo.ipynb). Unfortunately, one needs a non-negligeable amount of scaffolding for more realistic applications. Instead here we focus on a fully self-contained implementation.\n",
        "\n",
        "We start by defining our denoiser model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1nL_t8NJcrz"
      },
      "outputs": [],
      "source": [
        "class Denoiser(nn.Module):\n",
        "  \"\"\"A simple model MLPs and a Fourier Embedding.\"\"\"\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, x, sigma):\n",
        "    in_size = x.shape[1]\n",
        "    n_hidden = 256\n",
        "    sigma = jnp.concatenate([sigma,\n",
        "                             jnp.cos(2 * jnp.pi * sigma),\n",
        "                             jnp.sin(2 * jnp.pi * sigma),\n",
        "                             jnp.cos(4 * jnp.pi * sigma),\n",
        "                             jnp.sin(4 * jnp.pi * sigma)], axis=1)\n",
        "    x = jnp.concatenate([x, sigma], axis=1)\n",
        "    x = nn.Dense(n_hidden)(x)\n",
        "    x = nn.selu(x)\n",
        "    x = nn.Dense(n_hidden)(x)\n",
        "    x = nn.selu(x)\n",
        "    x = nn.Dense(n_hidden)(x)\n",
        "    x = nn.selu(x)\n",
        "    x = nn.Dense(in_size)(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElCIi-IwVAmP"
      },
      "source": [
        "As before, we consider our schedule, which provides the expressions for $f$ and $g$, and also for the scaling factor $s(t)$ and the noise level $\\sigma(t)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H893MPfbS79n"
      },
      "outputs": [],
      "source": [
        "beta_min = 0.001\n",
        "beta_max = 3\n",
        "\n",
        "def beta(t: jax.Array) -\u003e jax.Array:\n",
        "  return beta_min + t*(beta_max - beta_min)\n",
        "\n",
        "def int_beta(t: jax.Array) -\u003e jax.Array:\n",
        "  \"\"\"Integral of beta from 0 to t.\"\"\"\n",
        "  return t*beta_min + 0.5 * t**2 * (beta_max - beta_min)\n",
        "\n",
        "def f(x: jax.Array, t: jax.Array) -\u003e jax.Array:\n",
        "  return -0.5*beta(t)*x\n",
        "\n",
        "def g(t: jax.Array) -\u003e jax.Array:\n",
        "  return jnp.sqrt(beta(t))\n",
        "\n",
        "def s(t: jax.Array) -\u003e jax.Array:\n",
        "  return jnp.exp(-0.5 * int_beta(t))\n",
        "\n",
        "def sigma2(t: jax.Array) -\u003e jax.Array:\n",
        "  return 1 - jnp.exp(-int_beta(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzlU1M_lVENf"
      },
      "source": [
        "Here we initialize the denoiser model and its parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx3m4yMGSZuS"
      },
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "# Define dummy data for initailization.\n",
        "x, time = jnp.zeros((batch_size, d)), jnp.ones((batch_size, 1))\n",
        "\n",
        "# Initialize the model weights.\n",
        "denoiser = Denoiser()\n",
        "params = denoiser.init(rng, x, time)\n",
        "\n",
        "# Initialize the optimizer and its state.\n",
        "optimizer = optax.adam(1e-4)\n",
        "opt_state = optimizer.init(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ey0-qkVI57"
      },
      "source": [
        "We create the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVnG-vO8SkUf"
      },
      "outputs": [],
      "source": [
        "def loss_fn(\n",
        "    params: PyTree, model: nn.Module, rng: jax.Array, batch: jax.Array\n",
        ") -\u003e jax.Array:\n",
        "  \"\"\"loss function to be used to train the model\n",
        "\n",
        "  Args:\n",
        "      params: the current weights of the model\n",
        "      model: the score function\n",
        "      rng: random number generator from jax\n",
        "      batch: a batch of samples from the training data, representing samples\n",
        "        from \\mu_text{data}, shape (J, N)\n",
        "\n",
        "  Returns:\n",
        "      The value of the loss function defined above.\n",
        "  \"\"\"\n",
        "  rng, step_rng = jax.random.split(rng)\n",
        "  batch_size = batch.shape[0]\n",
        "  num_steps = 100\n",
        "  t = jax.random.randint(step_rng, (batch_size, 1), 1, num_steps) / (\n",
        "      num_steps - 1\n",
        "  )\n",
        "  # Extract the standard deviation from the schedule.\n",
        "  std = jnp.sqrt(sigma(t))\n",
        "  rng, step_rng = jax.random.split(rng)\n",
        "  noise = jax.random.normal(step_rng, batch.shape)\n",
        "  xt = batch + noise * std\n",
        "  output = model.apply(params, xt, std)\n",
        "  loss = jnp.mean((batch - output) ** 2)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBuZMuWX_9Dv"
      },
      "source": [
        "And the update step, which will update the weights of the networks contained in the PyTree params (this is a tree that contains all the internal structure of the network, including the values of the weights)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtJlW7R6_8E-"
      },
      "outputs": [],
      "source": [
        "@partial(jax.jit, static_argnums=[4])\n",
        "def update_step(\n",
        "    params: PyTree,\n",
        "    rng: jax.Array,\n",
        "    batch: jax.Array,\n",
        "    opt_state: optax.OptState,\n",
        "    model: nn.Module,\n",
        ") -\u003e tuple[jax.Array, PyTree, optax.OptState]:\n",
        "  \"\"\"Updates the parameters based on the gradient of the loss function.\n",
        "\n",
        "   Takes the gradient of the loss function and updates the model weights\n",
        "   (params) using it.\n",
        "\n",
        "  Args:\n",
        "      params: The current weights of the model.\n",
        "      rng: Random seed from jax.\n",
        "      batch: A batch of samples from the training data, representing samples\n",
        "        from p_{\\text{data}}, shape (d, N)\n",
        "      opt_state: the internal state of the optimizer\n",
        "      model: the score function\n",
        "\n",
        "  Returns:\n",
        "   A tuple with the value of the loss function (for metrics), the new parameters\n",
        "   and the new\n",
        "    optimizer state\n",
        "  \"\"\"\n",
        "  val, grads = jax.value_and_grad(loss_fn)(params, model, rng, batch)\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return val, params, opt_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdmuCBe1AMXm"
      },
      "source": [
        "Now we can run a very simple training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8n2GP3jKTXLe"
      },
      "outputs": [],
      "source": [
        "N_epochs = 3_000\n",
        "train_size = samples.shape[0]\n",
        "batch_size = 20\n",
        "steps_per_epoch = train_size // batch_size\n",
        "for k in tqdm(range(N_epochs)):\n",
        "  rng, step_rng = jax.random.split(rng)\n",
        "  # Permutes the data at each epoch.\n",
        "  data = jax.random.permutation(step_rng, samples, axis=0)\n",
        "  data = jnp.reshape(data, (steps_per_epoch, batch_size, data.shape[-1]))\n",
        "  losses = []\n",
        "  for batch in data:\n",
        "    rng, step_rng = jax.random.split(rng)\n",
        "    loss, params, opt_state = update_step(\n",
        "        params, step_rng, batch, opt_state, denoiser\n",
        "    )\n",
        "    losses.append(loss)\n",
        "  mean_loss = jnp.mean(jnp.array(losses))\n",
        "  if k % 1000 == 0:\n",
        "    print(\"Epoch %d \\t, Mean Loss %f \" % (k, mean_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vZQOQjkBCMN"
      },
      "source": [
        "We define the score fucntion by leveraing the proper normalization stemming from the choice of $f$ ang $g$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1KjL68XUbAp"
      },
      "outputs": [],
      "source": [
        "def trained_score(x, t):\n",
        "  x_t = x/s(t)\n",
        "  return (s(t) * denoiser.apply(params, x_t, jnp.sqrt(sigma2(t))) - x) / sigma2(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoUc0noIXJjw"
      },
      "source": [
        "Now we need to define a different solver that solves the SDE backwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0WZz_7fVqV-"
      },
      "outputs": [],
      "source": [
        "R = 1000\n",
        "train_ts = jnp.arange(1, R)/(R-1)\n",
        "\n",
        "@partial(jax.jit, static_argnums=(1,2,3,4,5))\n",
        "def sde_solver_backwards(key:jax.Array,\n",
        "                grad_log: Callable[[jax.Array], jax.Array],\n",
        "                g: Callable[[jax.Array], jax.Array],\n",
        "                f: Callable[[jax.Array, jax.Array], jax.Array],\n",
        "                N: int, n_samples: int,\n",
        "                ts: jax.Array=train_ts) -\u003e jax.Array:\n",
        "    \"\"\" Euler-Maruyama solver for the backwards SDE.\n",
        "\n",
        "    Args:\n",
        "        key: random number generator\n",
        "        grad_log: drift term for the SDE (here the score)\n",
        "        N: dimension of the problem\n",
        "        dt: time step\n",
        "        x_0: initial point\n",
        "    \"\"\"\n",
        "    def lmc_step_with_kernel(carry, params_time):\n",
        "        key, x = carry\n",
        "        t, dt = params_time\n",
        "        key, subkey = jax.random.split(key)\n",
        "        # Now we run the kernel using Euler-Maruyama\n",
        "        disp = g(1-t)\n",
        "        t = jnp.ones((x.shape[0], 1)) * t\n",
        "        drift = -f(x, 1-t) + grad_log(x, 1 - t)*disp**2\n",
        "        x = x + dt*drift + jnp.sqrt(dt)*disp*jax.random.normal(key=subkey, shape=(x.shape))\n",
        "        return (key, x), ()\n",
        "    key, subkey = jax.random.split(key)\n",
        "    dts = ts[1:] - ts[:-1]\n",
        "    params_time = jnp.stack([ts[:-1], dts], axis=1)\n",
        "    # Sampling the terminal condition with the correct std.\n",
        "    x_1 = jnp.sqrt(sigma2(1.)) * jax.random.normal(subkey, shape=(n_samples, N))\n",
        "    carry = (key, x_1)\n",
        "    (_, samples), _ = jax.lax.scan(lmc_step_with_kernel, carry, params_time)\n",
        "    return x_1, samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37esUuCvW-XU"
      },
      "source": [
        "Now we can use our learned score function (which is estimated from samples) to run the"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6j3B5FIdV6sA"
      },
      "outputs": [],
      "source": [
        "x_1, gen_samples = sde_solver_backwards(step_rng, trained_score, g, f, 3, 5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1C78gk-ciZ5"
      },
      "source": [
        "And we can plot the generated samples. Where we observe that we were able to capture the general geometry of the distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koLC4sDvWrVZ"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Scatter3d(x=gen_samples[:,0], y=gen_samples[:,1], z=gen_samples[:,2],\n",
        "                                   mode='markers',\n",
        "                             marker=dict(size=3,opacity=0.8)),])\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk7Tt-Qzd5QV"
      },
      "source": [
        "In this case we can also see the samples from the easy-to-sample distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3EDf1F_dXKj"
      },
      "outputs": [],
      "source": [
        "fig = go.Figure(data=[go.Scatter3d(x=gen_samples[:,0], y=gen_samples[:,1], z=gen_samples[:,2],\n",
        "                                   mode='markers',\n",
        "                             marker=dict(size=3,opacity=0.8)),\n",
        "                go.Scatter3d(x=x_1[:,0], y=x_1[:,1], z=x_1[:,2],\n",
        "                                   mode='markers',\n",
        "                             marker=dict(size=3,opacity=0.4)),])\n",
        "fig.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "17FKTcRxCQKpgOG3YSQYSOjf7CRXp-uwx",
          "timestamp": 1739494202001
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
