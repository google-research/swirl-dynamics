{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7gRI09mr_NE"
      },
      "source": [
        "## Memorization Tutorial\n",
        "\n",
        "In this colab we seek to showcase an example of memorization, in which a diffusion model is only able to memorize the training dataset, while failing the learn the underlying distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pjo8yCUcUmI"
      },
      "source": [
        "## Recap: How Score-based generative models work?\n",
        "Given access to samples $\\{x_i\\}$ from a target distribution with *unknown* probability distribution $p_{\\text{data}}$, generative models seek to learn the underlying distribution. The distribution in learn implicitly by learning a sampler, which draws samples from $p_{\\text{data}}$.\n",
        "\n",
        "Here we will use a simple low-dimensional distribution, and we will try to learn it from some of its samples, using a *naive*, albeit intuitive, approach to learn the score function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqM5ZP4fBq98"
      },
      "source": [
        "### Forward SDE\n",
        "Score-based generative models (SGM) are able to draw samples from the underlying distribution by leveraging the properties of a diffusive stochastic differential equation, which transform any distribution to a Gaussian distribution. Such equation is usually called the **forward SDE**, which then we will seek to **solve backwards**, thus effectively transforming samples from a Gaussian distribution to samples of our target one.\n",
        "\n",
        "Under a few regularity assumptions, any SDE of the form\n",
        "$$\\begin{array}{rcl}\n",
        "        \\mathrm{d}X_t \u0026 = \u0026 f(X_t,t)\\mathrm{d}t + g(t)\\mathrm{d}W_t \\\\\n",
        "        X_0 \u0026 \\sim \u0026 p_{\\text{data}}.\n",
        "\\end{array}\n",
        "$$\n",
        "with *drift coefficient* $f$ and *diffusion coefficient* $g$, has a corresponding Fokker-Planck equation given by\n",
        "$$\\partial_{t}p(x,t) = \\nabla_{x}\\cdot(f(x,t)p(x,t)) + \\frac{1}{2}g(t)^{2}\\Delta_{x} p(x,t).$$\n",
        "\n",
        "From now on denote the marginal distribution $p_{t}(x) := p(x,t)$ and we note that $p_{0} = \\mu_{\\text{data}}$. We suppose that we solve this SDE until a terminal time $T=1$.\n",
        "\n",
        "### Reverse SDE\n",
        "Fortunately, the previous SDE has the following **reverse-time SDE**\n",
        "$$\\begin{array}{rcl}\n",
        "        \\mathrm{d}X_t \u0026 = \u0026 -[f(X_t,t) + g(t)^2\\nabla \\log{p(X_t, t)}]\\mathrm{d}t + g(t)\\mathrm{d} \\bar{W}_t,\\\\\n",
        "        Y_1 \u0026 \\sim \u0026 N(0, \\sigma^2_{\\text{max}}),\n",
        "\\end{array}$$\n",
        "where,  $\\nabla_x \\log p(x, t)$ is called the *score function*, and $\\bar{W}_t$ is a backward Wiener process.\n",
        "\n",
        "Thus, if we know $p(x, t)$ then we could run the reverse SDE and generate samples from $X_0 \\sim p_{\\text{data}}$.\n",
        "\n",
        "### Computing the score function.\n",
        "\n",
        "Since $p_0 = p_{\\text{data}}$ is unknown, we do not have access to the marginals $p_t$ of the forward SDE. From the expression above, having access to such marginals (or an estimate of them) is a pre-requisite for solving the SDE.\n",
        "\n",
        "One simple way of estimating $\\nabla_x \\log p(x, t)$ is to use the empirial distribution given by dirac deltas centered at each datapoint, and then use the properties of the SDE to compute the marginals induced by evolving the SDE forward in time using the empirical measure as initial condition.\n",
        "\n",
        "This provides a very simple formula for the score function, but it leads to the phenomenon of **memorization** which we seek to showcase in this colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w3vHACyhMvi"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxS0AHP-hLUI"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Callable, Union\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9VJFijtfxju"
      },
      "source": [
        "### Samples from $p_{\\text{data}}$.\n",
        "\n",
        "For simplicity we consider a simple 2-dimensional distribution: a uniform distribution supported on a circle. We will draw a different amount of samples from this distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI1sQFfPgsSE"
      },
      "outputs": [],
      "source": [
        "def sample_circle(n_samples: int) -\u003e jax.Array:\n",
        "  \"\"\" Generates N samples from a 2d circle.\n",
        "\n",
        "  Args:\n",
        "    n_samples: Number of samples to be generated.\n",
        "\n",
        "  Returns:\n",
        "    A jax.Array of shape (n_samples, 2) containing the samples.\n",
        "  \"\"\"\n",
        "  alphas = jnp.linspace(0, 2*jnp.pi * (1 - 1/n_samples), n_samples)\n",
        "  xs = jnp.cos(alphas)\n",
        "  ys = jnp.sin(alphas)\n",
        "  mf = jnp.stack([xs, ys], axis=1)\n",
        "  return mf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7NxIXhWhAKq"
      },
      "source": [
        "We generate 8 samples and we plot them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaL9QZjxhDcb"
      },
      "outputs": [],
      "source": [
        "n_samples = 8\n",
        "mf = sample_circle(n_samples)\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.scatter(mf[:, 0], mf[:, 1])\n",
        "plt.axis('equal')\n",
        "plt.title(r'Samples from $p_{\\text{data}}$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYfOz1O4iEoJ"
      },
      "source": [
        "### Defining the Noising Process\n",
        "\n",
        "For simplicity we will use specific choices of $f$ and $g$. Namely, we define a function\n",
        "$$\\beta(t) = \\beta_\\text{min} + t(\\beta_\\text{max} - \\beta_\\text{min}),$$\n",
        "for $\\beta_\\text{min} = 0.001, \\beta_\\text{max} = 3$.\n",
        "\n",
        "We consider then\n",
        "$$f(t) := -\\frac{1}{2} \\beta(t) \\qquad \\text{and} \\qquad  g(t) := \\sqrt{\\beta(t)}$$\n",
        "\n",
        "Now, following the computation for the equivalent description of the noising process using a rescaling factor $s(t)$ and a noising factor $\\sigma(t)$, we have that\n",
        "$$s(t) = \\exp\\left ( - \\frac{1}{2}  \\int_{0}^t \\beta(s) ds \\right),$$\n",
        "and\n",
        "$$\\sigma^2(t) = \\int_0^t \\frac{g^2(\\xi)}{s^2(\\xi)} d\\xi = \\int_0^t \\frac{\\beta(\\xi)}{s(\\xi)} d\\xi = \\int_0^t \\beta(\\xi)\\exp\\left (\\int_{0}^{\\xi} \\beta(s) ds \\right) d\\xi = 1 - \\exp\\left ( - \\int_{0}^{t} \\beta(s) ds \\right).$$\n",
        "\n",
        "We define these function in what follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rMmBQO0k3dT"
      },
      "outputs": [],
      "source": [
        "beta_min = 0.001\n",
        "beta_max = 3\n",
        "\n",
        "def beta(t: jax.Array) -\u003e jax.Array:\n",
        "  return beta_min + t*(beta_max - beta_min)\n",
        "\n",
        "def int_beta(t: jax.Array) -\u003e jax.Array:\n",
        "  \"\"\"Integral of beta from 0 to t.\"\"\"\n",
        "  return t*beta_min + 0.5 * t**2 * (beta_max - beta_min)\n",
        "\n",
        "def f(x: jax.Array, t: jax.Array) -\u003e jax.Array:\n",
        "  return -0.5*beta(t)*x\n",
        "\n",
        "def g(t: jax.Array) -\u003e jax.Array:\n",
        "  return jnp.sqrt(beta(t))\n",
        "\n",
        "def s(t: jax.Array) -\u003e jax.Array:\n",
        "  return jnp.exp(-0.5 * int_beta(t))\n",
        "\n",
        "def sigma2(t: jax.Array) -\u003e jax.Array:\n",
        "  return 1 - jnp.exp(-int_beta(t))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p7IkKXPdegT"
      },
      "source": [
        "### Computing the marginal of the forward SDE.\n",
        "\n",
        "We have seen above that the time-$t$ transition kernel is given as\n",
        "$$p(x_t | x_0, t) = N(s(t) x_0, \\sigma(t)^2 I).$$\n",
        "\n",
        "We now assume that we have $N$ samples $\\{x^i\\}_{i=1}^N$ from our target distribution $p_{\\text{data}}$.\n",
        "\n",
        "The empirical measure\n",
        "$$p^N_0(x) = \\frac{1}{N}\\sum_{i =0}^{N} \\delta_{x_i}(x).$$\n",
        "is then an approximation to $p_{\\text{data}}$. If we start the forward SDE in $p_0 \\approx p^N_0$, we get marginals $p^{N}(x, t)$ given by,\n",
        "\n",
        "$$p^N(x, t) = \\frac{1}{N}\\sum_{i =0}^{N} N(x; s(t)x_i, \\sigma^2(t)I).,$$\n",
        "\n",
        "which is nothing more than a Gaussian mixture with $N$ components, one for each sample $x_i$. Each component of the mixture, is centred at $s(t) x_i$ and have variance $\\sigma^2(t)$.\n",
        "\n",
        "Therefore we can also actually write down the empirical score function $\\nabla \\log p^{N}_t$ (all though every evaluation of it needs to access the whole training set!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgK4KPa6cuRq"
      },
      "outputs": [],
      "source": [
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "def build_log_p(x_0: jax.Array) -\u003e Callable[[jax.Array, jax.Array], jax.Array]:\n",
        "  \"\"\" Builds the log density log p^N(x, t) using the empirical distribution.\n",
        "  Args:\n",
        "\n",
        "  x_0: Samples from the target distribution.\n",
        "\n",
        "  Reurns:\n",
        "    The log density log p^N(x, t) as described above.\n",
        "\n",
        "  \"\"\"\n",
        "  N = x_0.shape[0]\n",
        "  def log_p(x: jax.Array,  t: jax.Array) -\u003e jax.Array:\n",
        "    means = x_0 * s(t)\n",
        "    v = sigma2(t)\n",
        "    potentials = jnp.sum(-(x - means)**2 / (2 * v), axis=1)\n",
        "    return logsumexp(potentials, axis=0, b=1/N)\n",
        "    # this is equivalent to\n",
        "    # return jnp.log(1/N * jnp.sum(jnp.exp(potentials)))\n",
        "    # but is numerically more stable\n",
        "  return log_p\n",
        "\n",
        "log_p = build_log_p(mf)\n",
        "\n",
        "nabla_log_hat_pt = jax.jit(\n",
        "    jax.vmap(jax.grad(log_p), in_axes=(0, 0), out_axes=(0))\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxtAN0Ibn7Ny"
      },
      "source": [
        "Following our intuition from the Langevin dynamics. The score function should be pointing towards the support of the distribution. We visualize this on what follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nklK7r_Hdd6I"
      },
      "outputs": [],
      "source": [
        "def plot_score(\n",
        "    score: Callable[[jax.Array], jax.Array],\n",
        "    t: jax.Array,\n",
        "    area_min: float=-1.,\n",
        "    area_max: float=1.,\n",
        "    data_samples: jax.Array | None = None\n",
        ") -\u003e None:\n",
        "  \"\"\"\n",
        "  Plots the score function and optionally overlays data samples.\n",
        "\n",
        "  Args:\n",
        "    score: A callable function that takes a position and a time, and returns\n",
        "      the score evaluated at that position.\n",
        "    t: The time value at which to evaluate the score function.\n",
        "    area_min: The minimum value for the x and y axes of the plot.\n",
        "    area_max: The maximum value for the x and y axes of the plot.\n",
        "    data_samples: Optional jax.Array of data samples to plot.\n",
        "  \"\"\"\n",
        "  @partial(jax.jit, static_argnums=[0,])\n",
        "  def _helper(\n",
        "    score: Callable[[jax.Array, jax.Array], jax.Array],\n",
        "    t: jax.Array,\n",
        "    area_min: float,\n",
        "    area_max: float,\n",
        "  ) -\u003e tuple[jax.Array, jax.Array]:\n",
        "    x = jnp.linspace(area_min, area_max, 32)\n",
        "    x, y = jnp.meshgrid(x, x)\n",
        "    grid = jnp.stack([x.flatten(), y.flatten()], axis=1)\n",
        "    t = jnp.ones((grid.shape[0], 1)) * t\n",
        "    scores = score(grid, t)\n",
        "    return grid, scores\n",
        "\n",
        "  grid, scores = _helper(score, t, area_min, area_max)\n",
        "\n",
        "  plt.figure(figsize=(6, 6))\n",
        "  plt.quiver(\n",
        "    grid[:, 0],\n",
        "    grid[:, 1],\n",
        "    scores[:, 0],\n",
        "    scores[:, 1],\n",
        "    label=r\"$\\nabla_x \\log p^N$\",\n",
        "  )\n",
        "  plt.axis(\"equal\")\n",
        "\n",
        "  if data_samples is not None:  # To add the extra if necessary.\n",
        "    plt.scatter(\n",
        "      data_samples[:, 0],\n",
        "      data_samples[:, 1],\n",
        "      label=r\"Samples from $p_{\\text{data}}$\",\n",
        "    )\n",
        "\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plot_score(nabla_log_hat_pt, 0.005, -1.5, 1.5, data_samples=mf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb_5uWKsDvCf"
      },
      "source": [
        "Here we can observe where the main issue of this approach will arise: the flow points towards the support of the empirical distribution, and **not** the underlying one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6yi5raBAxla"
      },
      "source": [
        "To further visualize this problem, we consider the sampler that we built in the adjacent [notebook](https://github.com/google-research/swirl-dynamics/blob/main/swirl_dynamics/projects/probabilistic_diffusion/colabs/tutorial/diffusion_tutorial.ipynb) by solving the SDE backwards in time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ww-0UkbcFhNj"
      },
      "outputs": [],
      "source": [
        "def sde_solver_backwards(\n",
        "    key: jax.Array,\n",
        "    grad_log: Callable[[jax.Array, jax.Array], jax.Array],\n",
        "    g: Callable[[jax.Array], jax.Array],\n",
        "    f: Callable[[jax.Array, jax.Array], jax.Array],\n",
        "    dim: int,\n",
        "    n_samples: int,\n",
        "    num_time_steps: int = 100,\n",
        ") -\u003e jax.Array:\n",
        "    \"\"\"Euler-Maruyama solver for the backward SDE.\n",
        "\n",
        "    Args:\n",
        "        key: Seed for the random number generator.\n",
        "        grad_log: Drift term for the SDE (the score function).\n",
        "        g: Diffusion term for the SDE.\n",
        "        f: Drift term for the SDE.\n",
        "        dim: Dimension of the problem.\n",
        "        n_samples: Number of samples.\n",
        "        num_time_steps: Number of time steps.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the initial condition (x_1) and the sampled values.\n",
        "    \"\"\"\n",
        "\n",
        "    ts = jnp.linspace(1 / (num_time_steps - 1), 1, num_time_steps)\n",
        "    delta_t = ts[1:] - ts[:-1]\n",
        "\n",
        "    def time_step(\n",
        "          carry: tuple[jax.Array, jax.Array],\n",
        "          params_time: tuple[jax.Array, jax.Array]\n",
        "        )-\u003e tuple[tuple[jax.Array, jax.Array], None]:\n",
        "        \"\"\"Performs one step of the Euler-Maruyama.\"\"\"\n",
        "        key, x = carry\n",
        "        t, dt = params_time\n",
        "        key, subkey = random.split(key)\n",
        "\n",
        "        # Euler-Maruyama step\n",
        "        diff = g(1 - t)\n",
        "        t_broadcasted = jnp.ones((x.shape[0], 1)) * t\n",
        "        drift = -f(x, 1 - t_broadcasted) + grad_log(x, 1 - t_broadcasted) * diff**2\n",
        "        noise = random.normal(subkey, shape=x.shape)\n",
        "        x = x + dt * drift + jnp.sqrt(dt) * diff * noise\n",
        "        return (key, x), None  # We don't need to collect intermediate x\n",
        "\n",
        "    key, subkey = random.split(key)\n",
        "    sigma2_1 = sigma2(1.0)\n",
        "    x_1 = jnp.sqrt(sigma2_1) * random.normal(subkey, shape=(n_samples, dim))\n",
        "\n",
        "    carry = (key, x_1)\n",
        "    (_, samples), _ = jax.lax.scan(time_step, carry, jnp.stack([ts[:-1], delta_t], axis=1))\n",
        "    return x_1, samples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lL2hmwfqCVY"
      },
      "source": [
        "Now we can use this solution to sample from the underlying distributions using this approximation of the score function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgY08IqmABar"
      },
      "outputs": [],
      "source": [
        "rng, step_rng = random.split(jax.random.PRNGKey(0))\n",
        "x_1, gen_samples = sde_solver_backwards(step_rng, nabla_log_hat_pt, g, f, 2, 5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLf3QzbfpljP"
      },
      "outputs": [],
      "source": [
        "def plot_heatmap(positions: jax.Array,\n",
        "                 area_min: float = -2.,\n",
        "                 area_max: float = 2.) -\u003e None:\n",
        "  r\"\"\"Builds and plots a heatmap of the target distribution.\n",
        "\n",
        "  Args:\n",
        "      positions: Locations of all particles in $\\mathbb{R}^2$, array (N, 2).\n",
        "      area_min: Lowest x and y coordinates.\n",
        "      area_max: Highest x and y coordinates.\n",
        "\n",
        "  Returns:\n",
        "      None, but it will plot a heatmap of all particles in the area\n",
        "      [area_min, area_max] x [area_min, area_max].\n",
        "  \"\"\"\n",
        "  if area_min \u003e= area_max:\n",
        "    raise ValueError(\"area_min should be strictly lower than area_max.\")\n",
        "\n",
        "  @jax.jit\n",
        "  def produce_heatmap(\n",
        "    positions: jax.Array, area_min: float, area_max: float\n",
        "  )-\u003e jax.Array:\n",
        "    \"\"\"Generates the heatmap data from particle positions.\"\"\"\n",
        "    # Define the grid for the heatmap.\n",
        "    grid = jnp.linspace(area_min, area_max, 512)\n",
        "    x, y = jnp.meshgrid(grid, grid)\n",
        "\n",
        "    # Vectorized computation of distances.\n",
        "    x_pos = positions[:, 0]\n",
        "    y_pos = positions[:, 1]\n",
        "    dist = (x - x_pos[:, None, None])**2 + (y - y_pos[:, None, None])**2\n",
        "\n",
        "    # Vectorized computation of the heatmap contribution.\n",
        "    heatmap_values = jnp.exp(-350 * dist)\n",
        "\n",
        "    # Sum the contributions from all particles.\n",
        "    return jnp.sum(heatmap_values, axis=0)\n",
        "\n",
        "  # Generate the heatmap data.\n",
        "  heatmap_data = produce_heatmap(positions, area_min, area_max)\n",
        "\n",
        "  # Plot the heatmap.\n",
        "  extent = [area_min, area_max, area_min, area_max]  # Corrected extent\n",
        "  plt.imshow(\n",
        "    heatmap_data, cmap=\"coolwarm\", interpolation='nearest', extent=extent\n",
        "    )\n",
        "\n",
        "  # Invert the y-axis for proper orientation.\n",
        "  ax = plt.gca()\n",
        "  ax.invert_yaxis()\n",
        "\n",
        "  # Add labels and title.\n",
        "  plt.xlabel(\"X Coordinate\")\n",
        "  plt.ylabel(\"Y Coordinate\")\n",
        "  plt.title(\"Particle Heatmap\")\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpLERpoDrSnQ"
      },
      "source": [
        "First we look a the samples we used as initial (or terminal) conditions for the SDE. We can observe that follows a centered Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wap8iaVprQnd"
      },
      "outputs": [],
      "source": [
        "plot_heatmap(x_1, -2, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiOJ6jP1rgZI"
      },
      "source": [
        "However, the samples resulting from solving the SDE concentrate around the points already known. As a result, the sampler returns points from the training set, thus **memorizing** it instead of learning the underlying distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWu3ry9spoK0"
      },
      "outputs": [],
      "source": [
        "plot_heatmap(gen_samples, -3, 3)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1MyVE8CXknHt1kBvJCAIDTW1TpZxmaUni",
          "timestamp": 1740976972006
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
